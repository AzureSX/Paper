# 待解决问题

1. homo是啥？feature存放的是什么feature？user的？

   GCN、GAT、GraphSAGE 和 GeniePath 在同构图（即表 2 中的 Relation ALL）上运行，其中所有关系都合并在一起（怎么合并的？）。其他模型在多关系图上运行，它们在方法中处理来自不同关系的信息

   



# 参数设置

![image-20230413101231893](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230413101231893.png)+

![image-20230413101249768](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230413101249768.png)



# 加载数据

## load_data

```python
# load graph, feature, and label
# load_data函数是utils下的一个方法，入参是数据库名称
[homo, relation1, relation2, relation3], feat_data, labels = load_data(args.data)
```

```python
def load_data(data):
	"""
	Load graph, feature, and label given dataset name
	:returns: home and single-relation graphs, feature, label
	"""

	prefix = 'data/'										# 文件位置前缀

	"""
	labels是一个Numpy数组，它包含了所有评论的分类标签，而feat_data是一个Numpy矩阵，它包含了所有评论的特征数据。
	具体来说，feat_data矩阵的行数等于评论数，列数等于特征数，每个元素表示一个评论在一个特征上的取值
	"""

	if data == 'yelp':
        # loadmat函数专门用于加载.mat类型文件
		data_file = loadmat(prefix + 'YelpChi.mat')
        
        # x.flatten()是把numpy对象x降低到一维，默认是按照行来降维的
		labels = data_file['label'].flatten()
        
        # 首先通过调用todense()将其转换为稠密矩阵，然后再调用其A属性将其转换为Numpy数组
		feat_data = data_file['features'].todense().A
        
		# load the preprocessed adj_lists					加载预处理adj_lists
		with open(prefix + 'yelp_homo_adjlists.pickle', 'rb') as file:
			homo = pickle.load(file)
		file.close()
		with open(prefix + 'yelp_rur_adjlists.pickle', 'rb') as file:
			relation1 = pickle.load(file)
		file.close()
		with open(prefix + 'yelp_rtr_adjlists.pickle', 'rb') as file:
			relation2 = pickle.load(file)
		file.close()
		with open(prefix + 'yelp_rsr_adjlists.pickle', 'rb') as file:
			relation3 = pickle.load(file)
		file.close()
	# 如果输入的是amazon
	elif data == 'amazon':
		data_file = loadmat(prefix + 'Amazon.mat')
		labels = data_file['label'].flatten()
		feat_data = data_file['features'].todense().A
		# load the preprocessed adj_lists
		# 这一步将稀疏矩阵转换为邻接表
		with open(prefix + 'amz_homo_adjlists.pickle', 'rb') as file:
			homo = pickle.load(file)
		file.close()
		with open(prefix + 'amz_upu_adjlists.pickle', 'rb') as file:
			relation1 = pickle.load(file)
		file.close()
		with open(prefix + 'amz_usu_adjlists.pickle', 'rb') as file:
			relation2 = pickle.load(file)
		file.close()
		with open(prefix + 'amz_uvu_adjlists.pickle', 'rb') as file:
			relation3 = pickle.load(file)

	return [homo, relation1, relation2, relation3], feat_data, labels
```



```python
# yelp数据长这样，主要是几个关系矩阵、特征矩阵和一个标签数组
# 存储的都是稀疏矩阵和一个label数组，稀疏矩阵以csc格式存储

{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Aug 19 20:09:02 2020', '__version__': '1.0', '__globals__': [], 
 'homo': <45954x45954 sparse matrix of type '<class 'numpy.float64'>'with 7693958 stored elements in Compressed Sparse Column format>, 
 'net_rur': <45954x45954 sparse matrix of type '<class 'numpy.float64'>'with 98630 stored elements in Compressed Sparse Column format>, 
 'net_rtr': <45954x45954 sparse matrix of type '<class 'numpy.float64'>'with 1147232 stored elements in Compressed Sparse Column format>, 
 'net_rsr': <45954x45954 sparse matrix of type '<class 'numpy.float64'>'with 6805486 stored elements in Compressed Sparse Column format>, 
 'features': <45954x32 sparse matrix of type '<class 'numpy.float64'>'with 1469088 stored elements in Compressed Sparse Column format>, 
 'label': array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)}

```

![image-20230413110432847](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230413110432847.png)

## spare_to_adjlist

```python
	yelp = loadmat('data/YelpChi.mat')
	net_rur = yelp['net_rur']
	net_rtr = yelp['net_rtr']
	net_rsr = yelp['net_rsr']
	yelp_homo = yelp['homo']

	sparse_to_adjlist(net_rur, prefix + 'yelp_rur_adjlists.pickle')
	sparse_to_adjlist(net_rtr, prefix + 'yelp_rtr_adjlists.pickle')
	sparse_to_adjlist(net_rsr, prefix + 'yelp_rsr_adjlists.pickle')
	sparse_to_adjlist(yelp_homo, prefix + 'yelp_homo_adjlists.pickle')

	amz = loadmat('data/Amazon.mat')
	net_upu = amz['net_upu']
	net_usu = amz['net_usu']
	net_uvu = amz['net_uvu']
	amz_homo = amz['homo']

	sparse_to_adjlist(net_upu, prefix + 'amz_upu_adjlists.pickle')
	sparse_to_adjlist(net_usu, prefix + 'amz_usu_adjlists.pickle')
	sparse_to_adjlist(net_uvu, prefix + 'amz_uvu_adjlists.pickle')
	sparse_to_adjlist(amz_homo, prefix + 'amz_homo_adjlists.pickle')
```



```python
"""
以net_rur为例，这是一个稀疏矩阵
存储了由同一用户所发布的评论
"""

def sparse_to_adjlist(sp_matrix, filename):
	"""
	这段代码定义了一个函数sparse_to_adjlist，用于将稀疏矩阵转换成邻接表，并将结果保存到指定的文件中。
	函数接受两个参数：sp_matrix表示输入的稀疏矩阵，filename表示输出文件的路径和名称。
	函数首先将输入的稀疏矩阵加上了一个单位对角线，得到了一个有自环的邻接矩阵。
	然后，函数将邻接矩阵转换为邻接表，即对于每个节点，将其连接的节点保存到一个集合中，
	并将这些集合以节点的形式保存到一个字典中，最后将这个字典以二进制格式保存到指定的文件中，使用pickle库实现。

	函数的具体实现过程如下：
	先创建一个同构矩阵，即将原始稀疏矩阵和一个单位矩阵相加，使得每个节点都和自己形成一条边。
	创建一个空的字典，用于存储邻接表，键为节点编号，值为与该节点相邻的节点编号的集合。
	遍历同构矩阵中的所有非零元素，将节点之间的边添加到邻接表中。
	最后将邻接表存储到指定的文件中。
	
	"""
	# add self loop	
	homo_adj = sp_matrix + sp.eye(sp_matrix.shape[0])
    
	# 使用Python中的collections模块中的defaultdict类，初始化了一个类似于字典的对象adj_lists
	adj_lists = defaultdict(set)
    
	# homo_adj 是一个稀疏矩阵，并使用 nonzero() 方法找到该矩阵中所有非零元素的索引，并将其赋值给 edges 变量
    # 举例来说(array([0, 1, 2]), array([0, 1, 2])) 分别存储非零元素行列的索引
	edges = homo_adj.nonzero()
    
	# 对于每个端点，将其与另一个端点加入到对方的邻接列表中，这样就完成了邻接表的构建
    # 即一个邻接表中存放的是与一个节点所有相关联的节点
	for index, node in enumerate(edges[0]):
		adj_lists[node].add(edges[1][index])
		adj_lists[edges[1][index]].add(node)
	with open(filename, 'wb') as file: 
		pickle.dump(adj_lists, file)
	file.close()
```

代码例子展示

![image-20230413151846267](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230413151846267.png)

```py
# 上面加载得到的数据
[homo, relation1, relation2, relation3], feat_data, labels = load_data(args.data)


"""
homo: 
relation1(邻接表)
relation2(邻接表)
relation3(邻接表)
feat_data(特征矩阵)
labels(标签数组)
"""

```



# 训练测试分割

```python

"""
使用train_test_split函数将数据集划分成训练集和测试集
具体来说，将索引号列表index和标签列表labels作为输入，设置test_size=0.60表示测试集占总数据集的60%，并采用stratify参数使得划分后的训练集和测试集中标签类别的比例保持与原始数据集中相同，同时设置random_state=2以确保每次运行代码时得到相同的结果。最后返回训练集的索引号列表idx_train，测试集的索引号列表idx_test，训练集的标签列表y_train和测试集的标签列表y_test。
"""

if args.data == 'yelp':
    index = list(range(len(labels)))	# 构造一个标签数量大小的列表
    
    # train_test_split() sklearn函数 输入索引和标签，将原始索引和标签分为训练集和测试集
    idx_train, idx_test, y_train, y_test = train_test_split(index, labels, stratify=labels, test_size=0.60,random_state=2, shuffle=True)
    

```



# 下采样

```python
# split pos neg sets for under-sampling
train_pos, train_neg = pos_neg_split(idx_train, y_train)
```

## pos_neg_split()

```python
"""

这段代码定义了一个名为pos_neg_split()的函数，该函数接受两个参数：nodes和labels。其中，nodes是一个节点列表，labels是每个节点的标签列表。

该函数的作用是将节点列表按照标签分为正面节点和负面节点，并返回这两个节点列表。其中，正面节点的标签为1，负面节点的标签为0。

具体实现是通过遍历每个节点的标签，将其分别添加到正面节点列表或负面节点列表中。值得注意的是，为了避免修改原始节点列表，该函数使用了cp.deepcopy()来复制节点列表。

"""


def pos_neg_split(nodes, labels):
	"""
	Find positive and negative nodes given a list of nodes and their labels
	:param nodes: a list of nodes
	:param labels: a list of node labels
	:returns: the spited positive and negative nodes
	"""
	pos_nodes = []
	neg_nodes = cp.deepcopy(nodes)
	aux_nodes = cp.deepcopy(nodes)
	for idx, label in enumerate(labels):
		if label == 1:
			pos_nodes.append(aux_nodes[idx])
			neg_nodes.remove(aux_nodes[idx])

	return pos_nodes, neg_nodes
```



# 初始化模型输入

```python
"""
features = nn.Embedding(feat_data.shape[0], feat_data.shape[1]) 创建了一个嵌入层（embedding layer）features，该层将一个离散的特征映射为一个稠密的实数向量。其中，feat_data.shape[0] 表示特征的数量，feat_data.shape[1] 表示每个特征的维度。

nn.Embedding 通常用于自然语言处理（NLP）和推荐系统等领域的神经网络模型中，例如文本分类、语言建模、词嵌入、句子嵌入、推荐系统中用户和商品的嵌入等任务。它通常被用作输入层或嵌入层，将离散的特征（如词汇表中的单词或推荐系统中的用户ID和商品ID）映射到低维的实数向量空间中，从而方便神经网络对这些特征进行处理。

feat_data = normalize(feat_data) 对输入的特征数据 feat_data 进行归一化处理，以确保特征之间的尺度相当。这可以提高模型的性能和稳定性。

features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False) 将 features 的权重设置为一个大小为 (feat_data.shape[0], feat_data.shape[1]) 的张量，其中每行对应一个特征的稠密表示。 nn.Parameter 表示该张量可以被训练，而 requires_grad=False 表示在训练过程中不需要计算梯度。这将使得特征向量成为固定的输入，不会随着训练而更新。

if args.cuda: features.cuda()：根据命令行参数判断是否将数据传输到 GPU 上进行加速运算。如果 args.cuda 为真，则将嵌入层的参数放到 GPU 上。

在深度学习中，Embedding 是将离散型的特征值映射为连续型的向量表示的一种技术。它的作用主要有以下几个方面：

维度的降低和连续性的引入：在深度学习中，很多任务的输入数据都是离散型的，如单词、用户 ID 等，这些离散型的特征值不能直接输入到神经网络中进行处理。Embedding 可以将这些特征值映射为连续型的向量表示，从而实现了维度的降低和连续性的引入。

表示特征之间的相似度：Embedding 向量在连续空间中的距离可以表示出不同特征之间的相似度，这对于一些任务非常有用，如文本分类、推荐系统等。

降低稀疏性：在某些任务中，输入特征的取值非常稀疏，这样会导致模型的稀疏性增加，训练和推理的效率会降低。使用 Embedding 技术可以将稀疏性降低，提高模型的效率。

作为预训练模型的输入：Embedding 在自然语言处理等任务中广泛应用，如使用词向量表示单词、使用句子向量表示文本等。这些 Embedding 可以作为预训练模型的输入，提高模型的效果。


Embedding 和二维数组有以下几点不同：

Embedding 是将离散型的特征值映射为连续型的向量表示，而二维数组只是一种数据存储方式。二维数组中的每个元素都是离散的，而 Embedding 中的每个向量都是连续的。

Embedding 的每个向量是一个可训练参数，可以根据数据进行优化；而二维数组中的每个元素都是静态的，不会随着数据的变化而变化。

Embedding 通常用于处理高维稀疏特征，如文本中的单词、用户 ID 等；而二维数组通常用于处理低维密集特征，如图像的像素值。

Embedding 可以对输入特征进行降维处理，从而减少模型的参数数量，提高模型的效率；而二维数组则通常需要保留所有的输入特征，因为它们都是有意义的。

总之，Embedding 和二维数组都是用于处理数据的方式，但它们的应用场景和处理方式有很大的不同。Embedding 更加适用于处理高维稀疏特征，而二维数组则更适用于处理低维密集特征。

"""

# initialize model input

# nn.Embedding 是 PyTorch 中的一个模块，用于表示将整数索引映射到密集向量的查找表
# 这里随机生成了一个特征数量和维度大小的Embedding
features = nn.Embedding(feat_data.shape[0], feat_data.shape[1])

# 将特征矩阵元素归一化
feat_data = normalize(feat_data)

# 把归一化的特征矩阵的值tensor化，并赋值给feature，意义是把一个二维数组特征矩阵转化为embedding
features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)

if args.cuda:
    features.cuda()
```



# 设置输入图

```python
"""
设置输入的图和一些其他的参数
args.model默认是'CARE'
所以执行的是else
"""

# set input graph
if args.model == 'SAGE':
    adj_lists = homo
else:
    # 这是三个邻接表
    adj_lists = [relation1, relation2, relation3]

print(f'Model: {args.model}, Inter-AGG: {args.inter}, emb_size: {args.emb_size}.')
```





# 建立模型

```python
"""
建立模型
如果参数 args.model 的值为 'CARE'，则定义了三个 IntraAgg 层和一个 InterAgg 层，它们的输入是图的节点特征矩阵 features 和节点特征矩阵的列数 feat_data.shape[1]。

IntraAgg 层用于聚合节点自身的特征，
InterAgg 层用于聚合节点邻居的特征，并且在聚合过程中使用 IntraAgg 层得到的节点自身的特征作为辅助信息。
InterAgg 层将邻居节点的特征进行汇总，然后将聚合后的特征与节点自身的特征进行拼接，并通过一个线性层和激活函数进行处理，得到节点的嵌入向量
最后，使用 OneLayerCARE 模型将节点的嵌入向量转换为标签预测。

如果参数 args.model 的值为 'SAGE'，则定义了一个 MeanAggregator 层和一个 Encoder 层，其中 MeanAggregator 层用于聚合邻居节点的特征，Encoder 层用于对节点特征进行编码。具体来说，Encoder 层将节点自身的特征和邻居节点聚合后的特征进行拼接，然后通过一个线性层和激活函数进行处理，得到节点的嵌入向量。最后，使用 GraphSage 模型将节点的嵌入向量转换为标签预测。

在模型定义完成后，定义了一个 Adam 优化器，并使用训练数据进行训练，最终输出训练结果。

"""


# build one-layer models
if args.model == 'CARE':
    intra1 = IntraAgg(features, feat_data.shape[1], cuda=args.cuda)
    intra2 = IntraAgg(features, feat_data.shape[1], cuda=args.cuda)
    intra3 = IntraAgg(features, feat_data.shape[1], cuda=args.cuda)
    inter1 = InterAgg(features, feat_data.shape[1], args.emb_size, adj_lists, [intra1, intra2, intra3],
                      inter=args.inter,
                      step_size=args.step_size, cuda=args.cuda)
elif args.model == 'SAGE':
    agg1 = MeanAggregator(features, cuda=args.cuda)
    enc1 = Encoder(features, feat_data.shape[1], args.emb_size, adj_lists, agg1, gcn=True, cuda=args.cuda)

if args.model == 'CARE':
    gnn_model = OneLayerCARE(2, inter1, args.lambda_1)
elif args.model == 'SAGE':
    # the vanilla GraphSAGE model as baseline
    enc1.num_samples = 5
    gnn_model = GraphSage(2, enc1)

if args.cuda:
    gnn_model.cuda()

    """
    torch.optim.Adam是PyTorch中的Adam优化器，它可以自适应地调整学习率以及其他参数，使得模型能够更快地收敛到最优解。
	filter(lambda p: p.requires_grad, gnn_model.parameters())是一个过滤器，它过滤出了所有需要计算梯度的模型参	 数。在这里，我们只优化那些设置了requires_grad=True的参数，因为只有这些参数才需要进行梯度更新。
	lr=args.lr指定了学习率，即梯度下降的步长，通常需要通过实验来选择最优的学习率。
	weight_decay=args.lambda_2是正则化项，它可以惩罚过拟合的情况，防止模型在训练时过分依赖某些特征，导致泛化性能下降。	  这里的args.lambda_2是一个超参数，需要通过实验来确定最优值
    """
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, gnn_model.parameters()), lr=args.lr,
                             weight_decay=args.lambda_2)

# 时间和表现数组
times = []
performance_log = []
```



## InterAgg

```python
class InterAgg(nn.Module):

   def __init__(self, features, feature_dim,
             embed_dim, adj_lists, intraggs,
             inter='GNN', step_size=0.02, cuda=True):
      """
      Initialize the inter-relation aggregator
      :param features: the input node features or embeddings for all nodes
      :param feature_dim: the input dimension
      :param embed_dim: the output dimension
      :param adj_lists: a list of adjacency lists for each single-relation graph
      :param intraggs: the intra-relation aggregators used by each single-relation graph
      :param inter: the aggregator type: 'Att', 'Weight', 'Mean', 'GNN'
      :param step_size: the RL action step size
      :param cuda: whether to use GPU
      """
      super(InterAgg, self).__init__()

      self.features = features
      self.dropout = 0.6
      self.adj_lists = adj_lists
      self.intra_agg1 = intraggs[0]
      self.intra_agg2 = intraggs[1]
      self.intra_agg3 = intraggs[2]
      self.embed_dim = embed_dim
      self.feat_dim = feature_dim
      self.inter = inter
      self.step_size = step_size
      self.cuda = cuda
      self.intra_agg1.cuda = cuda
      self.intra_agg2.cuda = cuda
      self.intra_agg3.cuda = cuda

      # RL condition flag
      self.RL = True

      # number of batches for current epoch, assigned during training
      self.batch_num = 0

      # initial filtering thresholds
      self.thresholds = [0.5, 0.5, 0.5]

      # the activation function used by attention mechanism
      self.leakyrelu = nn.LeakyReLU(0.2)

      # parameter used to transform node embeddings before inter-relation aggregation
      """
      将权重张量标记为可训练的参数，这样在神经网络的训练过程中，PyTorch 就会自动计算权重张量的梯度，并利用梯度下降等优化		  算法更新权重的值。而 torch.FloatTensor 则指定了权重张量的数据类型，这里是浮点型
      接下来的 init.xavier_uniform_() 函数则是进行 Xavier 初始化的代码。Xavier 初始化是一种常用的神经网络参数初始		  化方法，它可以使得神经网络的训练更加稳定和快速。具体来说，Xavier 初始化会根据权重张量的形状和激活函数的类型，自动计	     算权重的标准差，然后按照一定的分布（通常是均匀分布或正态分布）对权重进行初始化。
      """
      self.weight = nn.Parameter(torch.FloatTensor(self.feat_dim, self.embed_dim))
      init.xavier_uniform_(self.weight)

      # weight parameter for each relation used by CARE-Weight
      self.alpha = nn.Parameter(torch.FloatTensor(self.embed_dim, 3))
      init.xavier_uniform_(self.alpha)

      # parameters used by attention layer
      self.a = nn.Parameter(torch.FloatTensor(2 * self.embed_dim, 1))
      init.xavier_uniform_(self.a)

      # label predictor for similarity measure
      self.label_clf = nn.Linear(self.feat_dim, 2)

      # initialize the parameter logs
      self.weights_log = []
      self.thresholds_log = [self.thresholds]
      self.relation_score_log = []

   def forward(self, nodes, labels, train_flag=True):
      """
      :param nodes: a list of batch node ids
      :param labels: a list of batch node labels, only used by the RLModule
      :param train_flag: indicates whether in training or testing mode
      :return combined: the embeddings of a batch of input node features
      :return center_scores: the label-aware scores of batch nodes
      """

      # extract 1-hop neighbor ids from adj lists of each single-relation graph
      # 基于每一种关系从adj_lists = [relation1, relation2, relation3]提取一跳邻居的id？
      # 新建一个邻居list
      to_neighs = []
      # 对adj_list循环提取每一种关系，
      for adj_list in self.adj_lists:
         to_neighs.append([set(adj_list[int(node)]) for node in nodes])

      # find unique nodes and their neighbors used in current batch
      # 这步在干嘛？这个unique_nodes有什么作用吗
      unique_nodes = set.union(set.union(*to_neighs[0]), set.union(*to_neighs[1]),
                         set.union(*to_neighs[2], set(nodes)))

      # calculate label-aware scores
      # 计算标签感知分数
      if self.cuda:
         # 根据上一步筛选出来的unique_nodes，从features中提取出batch中训练的nodes
         batch_features = self.features(torch.cuda.LongTensor(list(unique_nodes)))
      else:
         batch_features = self.features(torch.LongTensor(list(unique_nodes)))
            
      # self.label_clf = nn.Linear(self.feat_dim, 2)
      # 将 batch_features输入Linear模型，输出形状为(batch_siez, 2)的tensor
      batch_scores = self.label_clf(batch_features)
    
      # node_id和index的映射
      id_mapping = {node_id: index for node_id, index in zip(unique_nodes, range(len(unique_nodes)))}

      # the label-aware scores for current batch of nodes
      # 获取中心节点的分数
      center_scores = batch_scores[itemgetter(*nodes)(id_mapping), :]

      # get neighbor node id list for each batch node and relation
      # 获取邻居节点list
      r1_list = [list(to_neigh) for to_neigh in to_neighs[0]]
      r2_list = [list(to_neigh) for to_neigh in to_neighs[1]]
      r3_list = [list(to_neigh) for to_neigh in to_neighs[2]]

      # assign label-aware scores to neighbor nodes for each batch node and relation
      # 给每个关系下的邻居节点分配 label-aware 分数
      r1_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r1_list]
      r2_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r2_list]
      r3_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r3_list]

      # count the number of neighbors kept for aggregation for each batch node and relation
      # 默认thresholds是0.5，按照这个比例保留邻居，math.ceil()向上取整，存储的是每个节点的邻居个数
      r1_sample_num_list = [math.ceil(len(neighs) * self.thresholds[0]) for neighs in r1_list]
      r2_sample_num_list = [math.ceil(len(neighs) * self.thresholds[1]) for neighs in r2_list]
      r3_sample_num_list = [math.ceil(len(neighs) * self.thresholds[2]) for neighs in r3_list]

      # intra-aggregation steps for each relation
      # model() 和 model.forward() 从作用上来说是等价的
      # Eq. (8) in the paper
      # 传入batch_nodes、[r1,r2,r3]_list、center_scores、[r1,r2,r3]_scores、[r1,r2,r3]_sample_num_list
      r1_feats, r1_scores = self.intra_agg1.forward(nodes, r1_list, center_scores, r1_scores, r1_sample_num_list)
      r2_feats, r2_scores = self.intra_agg2.forward(nodes, r2_list, center_scores, r2_scores, r2_sample_num_list)
      r3_feats, r3_scores = self.intra_agg3.forward(nodes, r3_list, center_scores, r3_scores, r3_sample_num_list)

      # concat the intra-aggregated embeddings from each relation
      neigh_feats = torch.cat((r1_feats, r2_feats, r3_feats), dim=0)

      # get features or embeddings for batch nodes
      if self.cuda and isinstance(nodes, list):
         index = torch.LongTensor(nodes).cuda()
      else:
         index = torch.LongTensor(nodes)
      self_feats = self.features(index)

      # number of nodes in a batch
      n = len(nodes)

      # inter-relation aggregation steps
      # Eq. (9) in the paper
      if self.inter == 'Att':
         # 1) CARE-Att Inter-relation Aggregator
         combined, attention = att_inter_agg(len(self.adj_lists), self.leakyrelu, self_feats, neigh_feats, self.embed_dim,
                                    self.weight, self.a, n, self.dropout, self.training, self.cuda)
      elif self.inter == 'Weight':
         # 2) CARE-Weight Inter-relation Aggregator
         combined = weight_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, self.alpha, n, self.cuda)
         gem_weights = F.softmax(torch.sum(self.alpha, dim=0), dim=0).tolist()
         if train_flag:
            print(f'Weights: {gem_weights}')
      elif self.inter == 'Mean':
         # 3) CARE-Mean Inter-relation Aggregator
         combined = mean_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, n, self.cuda)
      elif self.inter == 'GNN':
         # 4) CARE-GNN Inter-relation Aggregator
         combined = threshold_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, self.thresholds, n, self.cuda)

      # the reinforcement learning module
      if self.RL and train_flag:
         relation_scores, rewards, thresholds, stop_flag = RLModule([r1_scores, r2_scores, r3_scores],
                                                      self.relation_score_log, labels, self.thresholds,
                                                      self.batch_num, self.step_size)
         self.thresholds = thresholds
         self.RL = stop_flag
         self.relation_score_log.append(relation_scores)
         self.thresholds_log.append(self.thresholds)

      return combined, center_scores
```



![image-20230417111418284](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230417111418284.png)



## IntraAgg

![image-20230414213441158](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230414213441158.png)

```python
"""

At the aggregation step, we first use the intra-relation aggregator to aggregate neighbor embeddings under each relation


"""



class IntraAgg(nn.Module):

    
	def __init__(self, features, feat_dim, cuda=False):
		"""
		Initialize the intra-relation aggregator
		:param features: the input node features or embeddings for all nodes
		:param feat_dim: the input dimension
		:param cuda: whether to use GPU
		"""
		super(IntraAgg, self).__init__()
		
        # 初始化IntraAgg模型时括号里输入的参数
        # 特征Embedding
		self.features = features
        # cpu计算
		self.cuda = cuda
        # 特征维度
		self.feat_dim = feat_dim

        # 变量名
	def forward(self, nodes, to_neighs_list, batch_scores, neigh_scores, sample_list):
		"""
		Code partially from https://github.com/williamleif/graphsage-simple/
		:param nodes: 一个批次里的节点list
		:param to_neighs_list: 一种关系的邻居节点list
		:param batch_scores: 一批节点的 label-aware 分数
		:param neigh_scores: 一种关系的邻居节点的 label-aware 分数
		:param sample_list: the number of neighbors kept for each batch node in one relation
		
		:return to_feats: the aggregated embeddings of batch nodes neighbors in one relation
		:return samp_scores: the average neighbor distances for each relation after filtering
		"""

		# filer neighbors under given relation
        # 给定关系下过滤后的邻居和分数
		samp_neighs, samp_scores = filter_neighs_ada_threshold(batch_scores, neigh_scores, to_neighs_list, sample_list)

		# find the unique nodes among batch nodes and the filtered neighbors
		unique_nodes_list = list(set.union(*samp_neighs))
		unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}

		# intra-relation aggregation only with sampled neighbors
		mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))
		column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]
		row_indices = [i for i in range(len(samp_neighs)) for _ in range(len(samp_neighs[i]))]
		mask[row_indices, column_indices] = 1
		if self.cuda:
			mask = mask.cuda()
		num_neigh = mask.sum(1, keepdim=True)
		mask = mask.div(num_neigh)
		if self.cuda:
			embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())
		else:
			embed_matrix = self.features(torch.LongTensor(unique_nodes_list))
		to_feats = mask.mm(embed_matrix)
		to_feats = F.relu(to_feats)
		return to_feats, samp_scores
```





## filter_neighs_ada_threshold

```python
# filer neighbors under given relation

# 在每一层采用单层 MLP 作为节点标签预测器，并使用两个节点预测结果之间的 l1 距离作为他们的相似性度量


		samp_neighs, samp_scores = filter_neighs_ada_threshold(batch_scores, neigh_scores, to_neighs_list, sample_list)
    
    
def filter_neighs_ada_threshold(center_scores, neigh_scores, neighs_list, sample_list):
	"""
	Filter neighbors according label predictor result with adaptive thresholds
	:param center_scores: the label-aware scores of batch nodes
	:param neigh_scores: the label-aware scores 1-hop neighbors each batch node in one relation
	:param neighs_list: neighbor node id list for each batch node in one relation
	:param sample_list: the number of neighbors kept for each batch node in one relation
	:return samp_neighs: the neighbor indices and neighbor simi scores
	:return samp_scores: the average neighbor distances for each relation after filtering
	
	使用自适应阈值根据标签预测结果过滤邻居节点
	:param center_scores: 批处理节点的标签感知得分
	:param neigh_scores: 一次关系中每个批处理节点的邻居节点的标签感知得分
	:param neighs_list: 一次关系中每个批处理节点的邻居节点ID列表
	:param sample_list: 每个批处理节点在一次关系中保留的邻居数量
	:return samp_neighs: 邻居索引和邻居相似度得分
	:return samp_scores: 过滤后每个关系的平均邻居距离得分
	
	"""
	# 新建两个数组变量
	samp_neighs = []
	samp_scores = []
    
    # center_socres是一个可遍历的对象
	for idx, center_score in enumerate(center_scores):
        # 以第一个遍历举例
        # 这个center_scores是一个batch_size x 2 的tenser，提取每行的第一列元素值
		center_score = center_scores[idx][0]
        
        """
        r1_scores[idx]：选择 r1_scores 张量中索引为 idx 的行
        [:, 0]：选择每行的第 0 列。这是 Python 切片语法，表示选择所有行（:），并且只选择第 0 列
        .view(-1, 1)：将结果转换为一个列向量。这是 PyTorch 中的方法
        这行代码的结果是从 r1_scores 张量中选取索引为 idx 的行，并将每行的第 0 列提取出来，形成一个列向量 					neigh_score。
        """
		neigh_score = neigh_scores[idx][:, 0].view(-1, 1)
        
        # 中心节点分数
        """
        这行代码的结果是将 center_score 张量沿着第 0 维进行复制，使得它的行数与 neigh_score 的行数相同，列数与 			neigh_score 的列数相同，然后将复制后的张量赋值给 center_score。这样，center_score 和 neigh_score 将具有		 相同的大小，可以用于后续的计算。
        """
		center_score = center_score.repeat(neigh_score.size()[0], 1)
        
        # 把 neighs_list[]中第idx个元素赋值给neighs_indices，是个数组
		neighs_indices = neighs_list[idx]
        # 把 sample_list[]中第idx个元素赋值给neighs_indices，是个数字
		num_sample = sample_list[idx]

		# compute the L1-distance of batch nodes and their neighbors
		# Eq. (2) in paper
        # 这里L_1就是绝对值，.squeeze()用于去除tensor中大小为 1 的维度，压缩tensor维度
		score_diff = torch.abs(center_score - neigh_score).squeeze()
        # 根据第0维的大小进行排序
		sorted_scores, sorted_indices = torch.sort(score_diff, dim=0, descending=False)
		selected_indices = sorted_indices.tolist()

		# top-p sampling according to distance ranking and thresholds
		# Section 3.3.1 in paper
		if len(neigh_scores[idx]) > num_sample + 1:
			selected_neighs = [neighs_indices[n] for n in selected_indices[:num_sample]]
			selected_scores = sorted_scores.tolist()[:num_sample]
		else:
			selected_neighs = neighs_indices
			selected_scores = score_diff.tolist()
			if isinstance(selected_scores, float):
				selected_scores = [selected_scores]

		samp_neighs.append(set(selected_neighs))
		samp_scores.append(selected_scores)

	return samp_neighs, samp_scores
```



# 训练模型

```python
# train the model

# 通过设定的epoch数目进行训练
for epoch in range(args.num_epochs):
    # randomly under-sampling negative nodes for each epoch
    # 随机下采样
    sampled_idx_train = undersample(train_pos, train_neg, scale=1)
    # random模块中的shuffle函数，对列表sampled_idx_train进行了随机打乱操作，即将该列表中的元素顺序随机地重新排列，以	达到随机化的目的
    rd.shuffle(sampled_idx_train)

    # send number of batches to model to let the RLModule know the training progress
    # 发送批次数量给模型，让RLModule知道训练进度。
    num_batches = int(len(sampled_idx_train) / args.batch_size) + 1
    if args.model == 'CARE':
        inter1.batch_num = num_batches

    loss = 0.0
    epoch_time = 0

    # mini-batch training
    for batch in range(num_batches):
        # 返回一个时间戳
        start_time = time.time()
        
        # 当前批次在原始数据中所对应的起始索引
        i_start = batch * args.batch_size
        # 当前批次在原始数据中所对应的终止索引
        i_end = min((batch + 1) * args.batch_size, len(sampled_idx_train))
        # 从训练集中去除对应起始和结尾的元素
        batch_nodes = sampled_idx_train[i_start:i_end]
        # 从标签集中去除对应起始和结尾的元素
        batch_label = labels[np.array(batch_nodes)]
        # 将网络的所有可训练参数的梯度值清零，以便于下一轮训练时重新计算梯度
        optimizer.zero_grad()
        # 如果有cpu
        if args.cuda:
            # 以 batch_nodes 和 batch_label 作为输入，计算得到当前批量样本的损失函数值
            # 这个方法会返回一个标量值,将这个值赋值给变量 loss
            loss = gnn_model.loss(batch_nodes, Variable(torch.cuda.LongTensor(batch_label)))
        else:
            loss = gnn_model.loss(batch_nodes, Variable(torch.LongTensor(batch_label)))
            
        """
        loss.backward() 是用来计算当前批量样本的损失函数对模型参数的梯度。计算得到的梯度值将被存储在模型参数的.grad 		 属性中。
		optimizer.step() 是用来更新模型参数的方法。它将使用优化器中定义的更新规则，对模型参数进行更新。具体而言，它将根		据模型参数的 .grad 属性的值，结合优化器中定义的学习率和正则化参数等参数，对模型参数进行更新。更新后，.grad 属性		  的值会被清零，以便进行下一次迭代。

		这两行代码通常在训练神经网络模型时放在一起使用。在反向传播计算完梯度之后，使用优化器更新模型参数。通过不断地迭代这		 两行代码，可以使得模型逐渐拟合训练数据，并最小化损失函数
        """
        loss.backward()
        optimizer.step()
        end_time = time.time()
        epoch_time += end_time - start_time
        # 将当前批次的损失值加到 loss 变量
        loss += loss.item()

    """
    epoch 表示当前的 epoch；
	loss.item() / num_batches 表示当前 epoch 的平均训练损失；
	epoch_time 表示当前 epoch 的训练时间。
    """
    print(f'Epoch: {epoch}, loss: {loss.item() / num_batches}, time: {epoch_time}s')
```



## undersample

```python
def undersample(pos_nodes, neg_nodes, scale=1):
   """
   Under-sample the negative nodes
   :param pos_nodes: a list of positive nodes
   :param neg_nodes: a list negative nodes
   :param scale: the under-sampling scale
   :return: a list of under-sampled batch nodes
   
   此代码的作用是对给定的正样本节点列表和负样本节点列表进行欠采样操作。欠采样是一种解决类别不平衡问题的方法，通过随机从多类    中选择一些样本进行下采样，以使得两个类别的样本数更加均衡。

   该函数的参数包括：pos_nodes：正样本节点列表，neg_nodes：负样本节点列表，scale：欠采样比例，默认值为1，表示正样本节    点和负样本节点数量相等。函数首先复制负样本节点列表，然后从该副本列表中随机选择一定数量的节点，使得正样本节点列表和欠采样    后的负样本节点列表数量比例为 scale:1，最后将两个列表合并成一个批次节点列表，并返回该列表。
   """

   aux_nodes = cp.deepcopy(neg_nodes)
   aux_nodes = rd.sample(aux_nodes, k=int(len(pos_nodes)*scale))
   batch_nodes = pos_nodes + aux_nodes

   return batch_nodes
```





## model嵌套

```python
#1 最终训练
loss = gnn_model.loss(batch_nodes, Variable(torch.cuda.LongTensor(batch_label)))

#2 gnn_model实例化
gnn_model = OneLayerCARE(2, inter1, args.lambda_1)

#3 OneLayerCARE
class OneLayerCARE(nn.Module):
	"""
	The CARE-GNN model in one layer
	"""

	def __init__(self, num_classes, inter1, lambda_1):
		"""
		Initialize the CARE-GNN model
		:param num_classes: number of classes (2 in our paper)
		:param inter1: the inter-relation aggregator that output the final embedding
		"""
		super(OneLayerCARE, self).__init__()
		self.inter1 = inter1
        
        # 设置Loss计算函数
		self.xent = nn.CrossEntropyLoss()

		# the parameter to transform the final embedding
    	# 定义了一个 大小为 64 X 2 的可训练tensor参数 why？
		self.weight = nn.Parameter(torch.FloatTensor(inter1.embed_dim, num_classes))
        # Xavier 均匀初始化方法
		init.xavier_uniform_(self.weight)
        # L_simi的lambda？
		self.lambda_1 = lambda_1

        # 输入节点，标签
        # #1中输入的 nodes = batch_nodes & labels = batch_label
	def forward(self, nodes, labels, train_flag=True):
        # 进入inter1
		embeds1, label_scores = self.inter1(nodes, labels, train_flag)
		scores = torch.mm(embeds1, self.weight)
		return scores, label_scores

	def to_prob(self, nodes, labels, train_flag=True):
		gnn_scores, label_scores = self.forward(nodes, labels, train_flag)
		gnn_prob = nn.functional.softmax(gnn_scores, dim=1)
		label_prob = nn.functional.softmax(label_scores, dim=1)
		return gnn_prob, label_prob

	def loss(self, nodes, labels, train_flag=True):
		gnn_scores, label_scores = self.forward(nodes, labels, train_flag)
		# Simi loss, Eq. (4) in the paper
		label_loss = self.xent(label_scores, labels.squeeze())
		# GNN loss, Eq. (10) in the paper
		gnn_loss = self.xent(gnn_scores, labels.squeeze())
		# the loss function of CARE-GNN, Eq. (11) in the paper
		final_loss = gnn_loss + self.lambda_1 * label_loss
		return final_loss
```



































# 测试模型

```python
# testing the model for every $test_epoch$ epoch

"""
这段代码的作用是在每个 test_epochs 轮后进行模型的测试，并将测试结果存储在 performance_log 列表中。其中：

if epoch % args.test_epochs == 0 表示在满足条件的情况下进行测试，条件是 epoch 能够被 args.test_epochs 整除；

idx_test 和 y_test 分别表示测试集的节点索引和标签；

gnn_model 表示我们训练好的 GNN 模型；

args.batch_size 表示测试过程中每个批次的大小；

test_sage 或 test_care 表示测试函数，具体使用哪个测试函数取决于模型的类型；

performance_log 是一个列表，用于存储测试结果，其中每个元素表示一次测试的结果，具体的测试指标包括 gnn_auc（GNN 模型的 AUC 值）、label_auc（标签传播模型的 AUC 值）、gnn_recall（GNN 模型的召回率）、label_recall（标签传播模型的召回率）。

"""


if epoch % args.test_epochs == 0:
    if args.model == 'SAGE':
        test_sage(idx_test, y_test, gnn_model, args.batch_size)
    else:
        gnn_auc, label_auc, gnn_recall, label_recall = test_care(idx_test, y_test, gnn_model, args.batch_size)
        performance_log.append([gnn_auc, label_auc, gnn_recall, label_recall])
```