

## init_seed()

```python
def init_seed():
    if 'reproducible' in configs['train']:
        if configs['train']['reproducible']:
            seed = configs['train']['seed']
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
```



## configs

```python
def parse_configure():
    # 命令行参数对象
    parser = argparse.ArgumentParser(description='SSLRec')
    # model：指定模型名
    parser.add_argument('--model', type=str, help='Model name')
    # dataset：指定数据集
    parser.add_argument('--dataset', type=str, default=None, help='Dataset name')
    # device：指定 cuda or cpu
    parser.add_argument('--device', type=str, default='cuda', help='cpu or cuda')
    # cuda：指定哪张显卡
    parser.add_argument('--cuda', type=str, default='0', help='Device number')
    # 获取命令行参数对象
    args = parser.parse_args()

    # 设定训练设备
    if args.device == 'cuda':
        os.environ['CUDA_VISIBLE_DEVICES'] = args.cuda
	# 没有输入模型报错
    if args.model == None:
        raise Exception("Please provide the model name through --model.")
    # 将输入的 model 小写
    model_name = args.model.lower()
    # 读取 model 对应的 yml 文件 
    if not os.path.exists('./config/modelconf/{}.yml'.format(model_name)):
        raise Exception("Please create the yaml file for your model first.")
    # 打开 yml 文件
    with open('./config/modelconf/{}.yml'.format(model_name), encoding='utf-8') as f:
        config_data = f.read()
        
        # 为后面提供参数
        configs = yaml.safe_load(config_data)

        # model name
        configs['model']['name'] = configs['model']['name'].lower()

        # grid search
        if 'tune' not in configs:
            configs['tune'] = {'enable': False}

        # gpu device
        configs['device'] = args.device

        # dataset
        if args.dataset is not None:
            configs['data']['name'] = args.dataset

        # log
        if 'log_loss' not in configs['train']:
            configs['train']['log_loss'] = True

        # early stop
        if 'patience' in configs['train']:
            if configs['train']['patience'] <= 0:
                raise Exception("'patience' should be greater than 0.")
            else:
                configs['train']['early_stop'] = True
        else:
            configs['train']['early_stop'] = False



        return configs

configs = parse_configure()
```

## build_data_handler()

```python
def build_data_handler():
    # 构建不同类型的数据集 e.g. 'data_handler_general_cf'
    datahandler_name = 'data_handler_' + configs['data']['type']
    # 根据任务类型选择对应的 handler e.g. 'data_utils.data_handler_general_cf'
    module_path = ".".join(['data_utils', datahandler_name])
    # 判断 module 是否存在
    if importlib.util.find_spec(module_path) is None:
        raise NotImplementedError('DataHandler {} is not implemented'.format(datahandler_name))
    # importlib 动态加载所需要的 module
    module = importlib.import_module(module_path)
    for attr in dir(module):
        # e.g. DataHandlerGeneralCF == 'data_handler_general_cf'
        if attr.lower() == datahandler_name.lower().replace('_', ''):
            return getattr(module, attr)()
    else:
        raise NotImplementedError('DataHandler Class {} is not defined in {}'.format(datahandler_name, module_path))
```

## class DataHandlerGeneralCF

```python
class DataHandlerGeneralCF:
    
    # 获取训练/验证/测试集 的相对地址
	def __init__(self):
        # 加载选择的数据集的地址
		if configs['data']['name'] == 'yelp':
			predir = './datasets/general_cf/sparse_yelp/'
		elif configs['data']['name'] == 'gowalla':
			predir = './datasets/general_cf/sparse_gowalla/'
		elif configs['data']['name'] == 'amazon':
			predir = './datasets/general_cf/sparse_amazon/'
		self.trn_file = predir + 'train_mat.pkl'
		self.val_file = predir + 'valid_mat.pkl'
		self.tst_file = predir + 'test_mat.pkl'

    # 加载邻接矩阵
	def _load_one_mat(self, file):
		"""Load one single adjacent matrix from file

		Args:
			file (string): path of the file to load

		Returns:
			scipy.sparse.coo_matrix: the loaded adjacent matrix
		"""
		with open(file, 'rb') as fs:
			mat = (pickle.load(fs) != 0).astype(np.float32)
		if type(mat) != coo_matrix:
			mat = coo_matrix(mat)
		return mat

    # 归一化邻接矩阵
	def _normalize_adj(self, mat):
		"""Laplacian normalization for mat in coo_matrix

		Args:
			mat (scipy.sparse.coo_matrix): the un-normalized adjacent matrix

		Returns:
			scipy.sparse.coo_matrix: normalized adjacent matrix
		"""
		# Add epsilon to avoid divide by zero
		degree = np.array(mat.sum(axis=-1)) + 1e-10
		d_inv_sqrt = np.reshape(np.power(degree, -0.5), [-1])
		d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0
		d_inv_sqrt_mat = sp.diags(d_inv_sqrt)
		return mat.dot(d_inv_sqrt_mat).transpose().dot(d_inv_sqrt_mat).tocoo()
	
    # 获得 tensor 类型的归一化邻接矩阵
	def _make_torch_adj(self, mat):
		"""Transform uni-directional adjacent matrix in coo_matrix into bi-directional adjacent matrix in torch.sparse.FloatTensor

		Args:
			mat (coo_matrix): the uni-directional adjacent matrix

		Returns:
			torch.sparse.FloatTensor: the bi-directional matrix
		"""
		a = csr_matrix((configs['data']['user_num'], configs['data']['user_num']))
		b = csr_matrix((configs['data']['item_num'], configs['data']['item_num']))
		mat = sp.vstack([sp.hstack([a, mat]), sp.hstack([mat.transpose(), b])])
		mat = (mat != 0) * 1.0
		# mat = (mat + sp.eye(mat.shape[0])) * 1.0# MARK
		mat = self._normalize_adj(mat)

		# make torch tensor
		idxs = t.from_numpy(np.vstack([mat.row, mat.col]).astype(np.int64))
		vals = t.from_numpy(mat.data.astype(np.float32))
		shape = t.Size(mat.shape)
		return t.sparse.FloatTensor(idxs, vals, shape).to(configs['device'])
	
	def load_data(self):
        # 先加载三种邻接矩阵
		trn_mat = self._load_one_mat(self.trn_file)
		tst_mat = self._load_one_mat(self.tst_file)
		val_mat = self._load_one_mat(self.val_file)

		self.trn_mat = trn_mat
        # 将 user_num 和 item_num 写入 configs 中
		configs['data']['user_num'], configs['data']['item_num'] = trn_mat.shape
        # 将原始矩阵转化为 tensor 可计算的矩阵
		self.torch_adj = self._make_torch_adj(trn_mat)

		if configs['train']['loss'] == 'pairwise':
            # 负采样
			trn_data = PairwiseTrnData(trn_mat)
		elif configs['train']['loss'] == 'pairwise_with_epoch_flag':
			trn_data = PairwiseWEpochFlagTrnData(trn_mat)
		# elif configs['train']['loss'] == 'pointwise':
		# 	trn_data = PointwiseTrnData(trn_mat)

        # 验证集/测试集包含了训练集
		val_data = AllRankTstData(val_mat, trn_mat)
		tst_data = AllRankTstData(tst_mat, trn_mat)
		self.valid_dataloader = data.DataLoader(val_data, batch_size=configs['test']['batch_size'], shuffle=False, num_workers=0)
		self.test_dataloader = data.DataLoader(tst_data, batch_size=configs['test']['batch_size'], shuffle=False, num_workers=0)
		self.train_dataloader = data.DataLoader(trn_data, batch_size=configs['train']['batch_size'], shuffle=True, num_workers=0)
```



## build_model(data_handler)

```python
def build_model(data_handler):
    # e.g. general_cf
    model_type = configs['data']['type']
    # e.g. gowalla
    model_name = configs['model']['name']
    # e.g. models.general_cf.lightgcn
    module_path = ".".join(['models', model_type, model_name])
    # 判断是否存在
    if importlib.util.find_spec(module_path) is None:
        raise NotImplementedError('Model {} is not implemented'.format(model_name))
    # 动态导入 module
    module = importlib.import_module(module_path)
    for attr in dir(module):
        if attr.lower() == model_name.lower():
            # 返回实例化后的对象，并传递参数
            return getattr(module, attr)(data_handler)
    else:
        raise NotImplementedError('Model Class {} is not defined in {}'.format(model_name, module_path))
```

## build_trainer(data_handler, logger)

```python
def build_trainer(data_handler, logger):
    # 指定训练器名字
    trainer_name = 'Trainer' if 'trainer' not in configs['train'] else configs['train']['trainer']
    # delete '_' in trainer name
    trainer_name = trainer_name.replace('_', '')
    # 从 trainer.trainer 中导入
    trainers = importlib.import_module('trainer.trainer')
    for attr in dir(trainers):
        # trainer
        if attr.lower() == trainer_name.lower():
            return getattr(trainers, attr)(data_handler, logger)
    else:
        raise NotImplementedError('Trainer Class {} is not defined in {}'.format(trainer_name, 'trainer.trainer'))
```



## trainer.train(mdoel)

```python
    def train(self, model):
        
        # 创建优化器
        self.create_optimizer(model)
        # 从 yml 文件中获取训练相关的设置
        train_config = configs['train']

        # 没有启动 early stop
        if not train_config['early_stop']:
            for epoch_idx in range(train_config['epoch']):
                # train
                self.train_epoch(model, epoch_idx)
                # evaluate
                if epoch_idx % train_config['test_step'] == 0:
                    self.evaluate(model, epoch_idx)
            self.test(model)
            self.save_model(model)
            return model
		
        # 启动了 early stop
        elif train_config['early_stop']:
            # 当前 patience
            now_patience = 0
            # 最好的 epoch
            best_epoch = 0
            # 最好的 metric
            best_metric = -1e9
            # 最好的 state_dict
            best_state_dict = None
            # 根据设定的最大 epoch 来进行训练
            for epoch_idx in range(train_config['epoch']):
                # train 将 model 和 epoch_idx 输入进行训练
                self.train_epoch(model, epoch_idx)
                # evaluate
                # 每几个 epoch 评估一次
                if epoch_idx % train_config['test_step'] == 0:
                    # 输出评估结果
                    eval_result = self.evaluate(model, epoch_idx)

                    if eval_result[configs['test']['metrics'][0]][0] > best_metric:
                        now_patience = 0
                        best_epoch = epoch_idx
                        best_metric = eval_result[configs['test']['metrics'][0]][0]
                        best_state_dict = deepcopy(model.state_dict())
                        self.logger.log("Validation score increased.  Copying the best model ...")
                    else:
                        now_patience += 1
                        self.logger.log(f"Early stop counter: {now_patience} out of {configs['train']['patience']}")

                    # early stop
                    if now_patience == configs['train']['patience']:
                        break

            # re-initialize the model and load the best parameter
            self.logger.log("Best Epoch {}".format(best_epoch))
            model = build_model(self.data_handler).to(configs['device'])
            model.load_state_dict(best_state_dict)
            self.evaluate(model)
            model = build_model(self.data_handler).to(configs['device'])
            model.load_state_dict(best_state_dict)
            self.test(model)
            self.save_model(model)
            return model
```



## train_epoch(self, model, epoch_idx)

```python
    def train_epoch(self, model, epoch_idx):
        
        # 加载训练数据
        # data.DataLoader(trn_data, batch_size=configs['train']['batch_size'], shuffle=True, num_workers=0)
        train_dataloader = self.data_handler.train_dataloader
        
        # 为每个 user 随机添加一个负样本
        train_dataloader.dataset.sample_negs()
        
        # 记录 loss
        loss_log_dict = {}
        
        # epoch loss
        ep_loss = 0
        
        # 获取步长，即一个 epoch 要训练多少 batch
        steps = len(train_dataloader.dataset) // configs['train']['batch_size']
        
        # 将模型设置为训练模式
        model.train()
        
        # 根据每个 batch 更新进度条
        for _, tem in tqdm(enumerate(train_dataloader), desc='Training Recommender', total=len(train_dataloader)):
            
            # 清零梯度
            self.optimizer.zero_grad()
            
            # 将数据批次 tem 中的每个元素都转换为长整数类型
            batch_data = list(map(lambda x: x.long().to(configs['device']), tem))
            
            # 计算单次 batch loss
            loss, loss_dict = model.cal_loss(batch_data)
            
            # 计入 epoch_loss
            ep_loss += loss.item()
            
            # 计算梯度
            loss.backward()
            
            # 根据梯度，更新参数
            self.optimizer.step()

            # record loss
            for loss_name in loss_dict:
                # 计算平均损失
                _loss_val = float(loss_dict[loss_name]) / len(train_dataloader)
                # 不存在就新建
                if loss_name not in loss_log_dict:
                    loss_log_dict[loss_name] = _loss_val
                # 存在就累加
                else:
                    loss_log_dict[loss_name] += _loss_val
		
        writer.add_scalar('Loss/train', ep_loss / steps, epoch_idx)

        # log loss
        if configs['train']['log_loss']:
            self.logger.log_loss(epoch_idx, loss_log_dict)
        else:
            self.logger.log_loss(epoch_idx, loss_log_dict, save_to_log=False)
```

## model.cal_loss(batch_data)

```python
	def cal_loss(self, batch_data):
        
		self.is_training = True
        
        # 通过传播获得 user_embeds, item_embeds
		user_embeds, item_embeds = self.forward(self.adj, self.keep_rate)
        
        # user item negs 的 indexs
		ancs, poss, negs = batch_data
		anc_embeds = user_embeds[ancs]
		pos_embeds = item_embeds[poss]
		neg_embeds = item_embeds[negs]
        
        # 计算 bpr_loss
		bpr_loss = cal_bpr_loss(anc_embeds, pos_embeds, neg_embeds) / anc_embeds.shape[0]
        
        # 计算 regularization loss
		reg_loss = self.reg_weight * reg_params(self)
        
        # loss 数值
		loss = bpr_loss + reg_loss
        
        # loss 对象
		losses = {'bpr_loss': bpr_loss, 'reg_loss': reg_loss}
		return loss, losses
```

## forward(self, adj, keep_rate)

```python
	def forward(self, adj, keep_rate):
        
        # 如果模型不处于训练状态且最终嵌入已经计算
		if not self.is_training and self.final_embeds is not None:
            # 返回 final_embeds
			return self.final_embeds[:self.user_num], self.final_embeds[self.user_num:]
        
        # 将 user_embeds 和 item_embeds 按行拼接
		embeds = t.concat([self.user_embeds, self.item_embeds], axis=0)
        
        # 将上一步组合的 embeds 放入 embeds_list 中
		embeds_list = [embeds]
        
        # 判断是否处于训练模式
		if self.is_training:
            # 为什么要 edge_drop? 原文应该是没有这个过程啊
            # EdgeDrop 类可以增加模型的鲁棒性、泛化能力
			adj = self.edge_dropper(adj, keep_rate)
        
        # 几层 GCN ?
		for i in range(self.layer_num):
            # 传播过程
			embeds = self._propagate(adj, embeds_list[-1])
            # 将传播后的 embeds 加入 embeds_list
			embeds_list.append(embeds)
        
        # readout
		embeds = sum(embeds_list)# / len(embeds_list)
        
        # final_embeds 是前几层 embs 通过某种形式组合得到
		self.final_embeds = embeds
        
        # 返回 user_embeds 和 item_embeds
		return embeds[:self.user_num], embeds[self.user_num:]
```

## _propagate(self, adj, embeds)

```python
	def _propagate(self, adj, embeds):
        
        # 归一化邻接矩阵和 embeds 相乘即是传播过程
		return t.spmm(adj, embeds)
```

## evaluate(self, model, epoch_idx=None)

```python
    def evaluate(self, model, epoch_idx=None):
        
        # model 设置为 eavl 状态
        model.eval()
        
        # 判断 self.data_handler 中是否存在 valid_dataloader
        if hasattr(self.data_handler, 'valid_dataloader'):
            
            eval_result = self.metric.eval(model, self.data_handler.valid_dataloader)
            writer.add_scalar('HR/test', eval_result[configs['test']['metrics'][0]][0], epoch_idx)
            self.logger.log_eval(eval_result, configs['test']['k'], data_type='Validation set', epoch_idx=epoch_idx)
        elif hasattr(self.data_handler, 'test_dataloader'):
            eval_result = self.metric.eval(model, self.data_handler.test_dataloader)
            writer.add_scalar('HR/test', eval_result[configs['test']['metrics'][0]][0], epoch_idx)
            self.logger.log_eval(eval_result, configs['test']['k'], data_type='Test set', epoch_idx=epoch_idx)
        else:
            raise NotImplemented
        return eval_result
```

## eval(self, model, test_dataloader)

```python
    def eval(self, model, test_dataloader):
        
        # for most GNN models, you can have all embeddings ready at one forward
        # 如果有 eval_at_one_forward 属性 且为 True 则跳转到另一函数
        if 'eval_at_one_forward' in configs['test'] and configs['test']['eval_at_one_forward']:
            return self.eval_at_one_forward(model, test_dataloader)
		
        # 创建 result 对象
        result = {}
        
        # 遍历 metrics，获得单个 metric
        for metric in self.metrics:
            # 根据 k 数组的大小创建相应大小的空数组 e.g. 
            result[metric] = np.zeros(len(self.k))

        # 用于存储模型生成的推荐评分
        batch_ratings = []
        
        # 用于存储真实的、已知的用户-物品交互评分
        ground_truths = []
        
        # 计数
        test_user_count = 0
        # 测试集中 user 的个数, 从 ALLRANKTstData 类中提取的 test_users 属性 user index 的集合
        test_user_num = len(test_dataloader.dataset.test_users)
        
        for _, tem in enumerate(test_dataloader):
            
            # 确保 tem 变量始终是一个列表类型的对象
            if not isinstance(tem, list):
                tem = [tem]
            
            # 返回的是一个 batch 中 user 的 index e.g. 0-1023
            test_user = tem[0].numpy().tolist()
            
            # 一个 list list[0] 是 user index list[1] 是valid_dataloader对应的交互 
            batch_data = list(
                map(lambda x: x.long().to(configs['device']), tem))
            
            # predict result
            with torch.no_grad():
                
                # 获得 batch 的预测值
                batch_pred = model.full_predict(batch_data)
            
            # 统计 test_user 数目
            test_user_count += batch_pred.shape[0]
            
            # 返回的可能是 batch_pred
            batch_pred = self._mask_history_pos(
                batch_pred, test_user, test_dataloader)
            
            # 返回 MAX-K 个最大值的索引
            _, batch_rate = torch.topk(batch_pred, k=max(self.k))
            
            # 将 tok-K 个索引加入 batch_ratings
            batch_ratings.append(batch_rate.cpu())
            
            # ground truth
            ground_truth = []
            
            # 根据 test_user 中的 index 循环
            for user_idx in test_user:
                
                # 向 ground_truth 中添加 test/valid 集中 user 交互过的 item 的 index
                # 从 user_pos_lists 中提取对应的 list
                ground_truth.append(
                    list(test_dataloader.dataset.user_pos_lists[user_idx]))
                
            # 添加到 ground_truths
            ground_truths.append(ground_truth)
            
        # 判断数量能否对应
        assert test_user_count == test_user_num

        # calculate metrics
        # 将 batch_ratings 和 ground_truths 按照索引一一配对
        data_pair = zip(batch_ratings, ground_truths)
        
        # 评估结果
        eval_results = []
        
        # 遍历 data_pair
        for _data in data_pair:
            eval_results.append(self.eval_batch(_data, self.k))
        for batch_result in eval_results:
            for metric in self.metrics:
                result[metric] += batch_result[metric] / test_user_num

        return result
```

## full_predict(self, batch_data)

```python
	def full_predict(self, batch_data):
        
        # embeds[:self.user_num], embeds[self.user_num:]
		user_embeds, item_embeds = self.forward(self.adj, 1.0)
        
        # 
		self.is_training = False
        
        # pck_users：users index，train_mask：每个 user 对应的 valid 交互
		pck_users, train_mask = batch_data
		pck_users = pck_users.long()
        
        # 从 user_embeds 提取当前 batch 的 user_embeds
		pck_user_embeds = user_embeds[pck_users]
        
        # 矩阵乘法计算评分
		full_preds = pck_user_embeds @ item_embeds.T
        
        # 返回排除掉训练集已经交互的元素之后的预测值
		full_preds = self._mask_predict(full_preds, train_mask)
		return full_preds
```

## _mask_predict(self, full_preds, train_mask)

```python
	def _mask_predict(self, full_preds, train_mask):
        # 首先将 train_mask 翻转，排除掉训练集中已经交互过的 item 的评分
        # 未知的交互对应的元素保留了其原始预测值，已知的交互对应的元素被置为一个非常大的负数
		return full_preds * (1 - train_mask) - 1e8 * train_mask
```

## _mask_history_pos(self, batch_rate, test_user, test_dataloader)

```python
    def _mask_history_pos(self, batch_rate, test_user, test_dataloader):
        
        # 检查 test_dataloader.dataset 对象是否具有名为user_history_lists的属性
        if not hasattr(test_dataloader.dataset, 'user_history_lists'):
            return batch_rate
        
        # 
        for i, user_idx in enumerate(test_user):
            pos_list = test_dataloader.dataset.user_history_lists[user_idx]
            batch_rate[i, pos_list] = -1e8
        return batch_rate
```

## eval_batch(self, data, topks)

```python
    
    # data：是 data_pair topks: [10,20,40]
    def eval_batch(self, data, topks):
        sorted_items = data[0].numpy()
        ground_true = data[1]
        r = self.get_label(ground_true, sorted_items)

        result = {}
        for metric in self.metrics:
            result[metric] = []

        for k in topks:
            for metric in result:
                if metric == 'recall':
                    result[metric].append(self.recall(ground_true, r, k))
                if metric == 'ndcg':
                    result[metric].append(self.ndcg(ground_true, r, k))
                if metric == 'precision':
                    result[metric].append(self.precision(r, k))
                if metric == 'mrr':
                    result[metric].append(self.mrr(r, k))

        for metric in result:
            result[metric] = np.array(result[metric])

        return result
```

## get_label(self, test_data, pred_data)

```python
    def get_label(self, test_data, pred_data):
        r = []
        for i in range(len(test_data)):
            ground_true = test_data[i]
            predict_topk = pred_data[i]
            pred = list(map(lambda x: x in ground_true, predict_topk))
            pred = np.array(pred).astype("float")
            r.append(pred)
        return np.array(r).astype('float')
```

