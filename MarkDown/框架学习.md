![image-20231109211222024](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231109211222024.png)

# config里都有啥

```python
dataset = 'amazon-book'
final_config_dict=
{'gpu_id': 0, 
 'use_gpu': True, 
 'seed': 2020, 
 'state': 'INFO', 
 'reproducibility': True, 
 'data_path': 'dataset/amazon-book', 
 'checkpoint_dir': 'saved', 
 'show_progress': True, 
 'save_dataset': False, 
 'dataset_save_path': None, 
 'save_dataloaders': False, 
 'dataloaders_save_path': None, 
 'log_wandb': False, 
 'wandb_project': 'recbole', 
 'epochs': 500, 
 'train_batch_size': 2048, 
 'learner': 'adam', 
 'learning_rate': 0.001, 
 'neg_sampling': {'uniform': 1}, 
 'eval_step': 1, 
 'stopping_step': 10, 
 'clip_grad_norm': None, 
 'weight_decay': 0.0, 
 'loss_decimal_place': 4, 
 'require_pow': False, 
 
 # 数据划分比例 训练集/验证集/测试集
 'eval_args': {'split': {'RS': [0.8, 0.1, 0.1]}, 'group_by': 'user', 'order': 'RO', 'mode': 'full'}, 
 
 'repeatable': False, 
 'metrics': ['Recall', 'NDCG'], 
 'topk': [10, 20, 50], 
 'valid_metric': 'NDCG@20', 
 'valid_metric_bigger': True, 
 'eval_batch_size': 4096000, 
 'metric_decimal_place': 4, 
 'field_separator': '\t', 
 'seq_separator': ' ',
 'USER_ID_FIELD': 'user_id', 
 'ITEM_ID_FIELD': 'item_id', 
 'RATING_FIELD': 'rating', 
 'TIME_FIELD': 'timestamp', 
 'seq_len': None, 
 'LABEL_FIELD': 'label', 
 'threshold': None, 
 'NEG_PREFIX': 'neg_', 
 'load_col': {'inter': ['user_id', 'item_id'], 'kg': ['head_id', 'relation_id', 'tail_id'], 'link': ['item_id', 'entity_id']}, 
 'unload_col': None, 
 'unused_col': None, 
 'additional_feat_suffix': None, 
 'rm_dup_inter': None, 
 'val_interval': None,
 'filter_inter_by_user_or_item': True, 
 'user_inter_num_interval': '[0,inf)', 
 'item_inter_num_interval': '[0,inf)', 
 'alias_of_user_id': None, 
 'alias_of_item_id': None, 
 'alias_of_entity_id': None, 
 'alias_of_relation_id': None, 
 'preload_weight': None, 
 'normalize_field': None, 
 'normalize_all': None, 
 'ITEM_LIST_LENGTH_FIELD': 'item_length', 
 'LIST_SUFFIX': '_list', 
 'MAX_ITEM_LIST_LENGTH': 50, 
 'POSITION_FIELD': 'position_id', 
 'HEAD_ENTITY_ID_FIELD': 'head_id', 
 'TAIL_ENTITY_ID_FIELD': 'tail_id', 
 'RELATION_ID_FIELD': 'relation_id', 
 'ENTITY_ID_FIELD': 'entity_id',
 'benchmark_filename': ['train', 'test', 'test'], 
 'MODEL_TYPE': <ModelType.KNOWLEDGE: 4>, 
 'log_wandb = Falseeval_setting': {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': 'full'}, 
 'reg_weight': 0.0001, 
 'embedding_size': 64, 
 'n_layers': 3, 
 'cl_rate': 0.5, 
 'mess_drop_rate': 0.1,
 'kg_drop_rate': 0.1, 
 'ig_drop_rate': 0.5, 
 'tau': 0.2, 
 'dataset': 'amazon-book', 
 'model': 'SimKGCL', 
 'MODEL_INPUT_TYPE': <InputType.PAIRWISE: 2>, 
 'eval_type': <EvaluatorType.RANKING: 1>, 
 'device': device(type='cuda'), 
 'train_neg_sample_args': {'strategy': 'by', 'by': 1, 'distribution': 'uniform', 'dynamic': 'none'}, 
 'eval_neg_sample_args': {'strategy': 'full', 'distribution': 'uniform'}
}
```

# info(config)

```python
General Hyper Parameters：

# GPU 编号
gpu_id = 0
# 是否使用 GPU
use_gpu = True
# 种子
seed = 2020
# 状态
state = INFO
# 可重复性
reproducibility = True
# 数据位置
data_path = dataset/amazon-book
# 中间状态
checkpoint_dir = saved
# 展示过程
show_progress = True
# 保存数据集
save_dataset = False
# 数据集保存路径
dataset_save_path = None
# 保存 dataloader
save_dataloaders = False
# dotaloader保存路径
dataloaders_save_path = None
# wandb 是调参工具
log_wandb = False

----------------------------------
Training Hyper Parameters:

# epochs 次数
epochs = 500
# batch_size
train_batch_size = 2048
# 学习器
learner = adam
# 学习率
learning_rate = 0.001
# 负采样
neg_sampling = {'uniform': 1}
# 评估步长
eval_step = 1
# 停止训练步长
stopping_step = 10
# 梯度裁剪
clip_grad_norm = None
# 权重衰减
weight_decay = 0.0
# Loss 的小数点
loss_decimal_place = 4

----------------------------------
Evaluation Hyper Parameters:

# 评估参数
eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'group_by': 'user', 'order': 'RO', 'mode': 'full'}
# 可重复
repeatable = False
# 评估指标
metrics = ['Recall', 'NDCG']
# topk
topk = [10, 20, 50]
# 验证指标
valid_metric = NDCG@20
# 不是很懂
valid_metric_bigger = True
# 验证 batch_size
eval_batch_size = 4096000
# 小数点
metric_decimal_place = 4

----------------------------------
Dataset Hyper Parameters:

# 无
field_separator = '\t'
# 无
seq_separator = 
# 用户标识符
USER_ID_FIELD = user_id
# 物品标识符
ITEM_ID_FIELD = item_id
# 评价标识符
RATING_FIELD = rating
# 时间标识符
TIME_FIELD = timestamp
# 序列长度
seq_len = None
# 标签
LABEL_FIELD = label
# 阈值
threshold = None
# 负样本前缀
NEG_PREFIX = neg_
# 加载列
load_col = {'inter': ['user_id', 'item_id'], 'kg': ['head_id', 'relation_id', 'tail_id'], 'link': ['item_id', 'entity_id']}
# 未加载列
unload_col = None
# 未使用列
unused_col = None
# 额外的特征前缀
additional_feat_suffix = None
# 移除重复的交互
rm_dup_inter = None
# 验证间隔
val_interval = None
# filter_inter_by_user_or_item
filter_inter_by_user_or_item = True
# 用户交互数间隔
user_inter_num_interval = [0,inf)
# 物品交互数间隔
item_inter_num_interval = [0,inf)
# 用户别名
alias_of_user_id = None
# 物品别名
alias_of_item_id = None
# 实体别名
alias_of_entity_id = None
# 关系别名
alias_of_relation_id = None
# 预加载权重
preload_weight = None
# 归一化
normalize_field = None
# 还是归一化
normalize_all = None
# 物品列表长度
ITEM_LIST_LENGTH_FIELD = item_length
# 列表后缀
LIST_SUFFIX = _list
# 最大物品列表长度
MAX_ITEM_LIST_LENGTH = 50
# POSITION_FIELD 有这个？
POSITION_FIELD = position_id
# 头实体
HEAD_ENTITY_ID_FIELD = head_id
# 尾实体
TAIL_ENTITY_ID_FIELD = tail_id
# 关系实体
RELATION_ID_FIELD = relation_id
# 实体
ENTITY_ID_FIELD = entity_id
# Benchmark 文件名
benchmark_filename = ['train', 'test', 'test']

----------------------------------
Other Hyper Parameters:
                           
wandb_project = recbole
require_pow = False
# 模型类型                           
MODEL_TYPE = ModelType.KNOWLEDGE
# 暂时用不上                           
log_wandb = Falseeval_setting = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': 'full'}
# 正则化
reg_weight = 0.0001
# 嵌入大小
embedding_size = 64
# 卷积层数
n_layers = 3
# cl 率
cl_rate = 0.5
# mess 丢失率
mess_drop_rate = 0.1
# KG 丢失率
kg_drop_rate = 0.1
# IG 丢失率
ig_drop_rate = 0.5
# 温度函数
tau = 0.2
# 模型输入类型
MODEL_INPUT_TYPE = InputType.PAIRWISE
# 评估类型
eval_type = EvaluatorType.RANKING
# 设备
device = cuda
# 训练负采样策略
train_neg_sample_args = {'strategy': 'by', 'by': 1, 'distribution': 'uniform', 'dynamic': 'none'}
# 评估负采样策略
eval_neg_sample_args = {'strategy': 'full', 'distribution': 'uniform'}                           
```



# dataset里都有啥

```python
alias = {'entity_id': ['head_id' 'tail_id'], 'item_id': ['item_id'], 'relation_id': ['relation_id'], 'user_id': ['user_id']}

avg_actions_of_items = 41.756130844872565

avg_actions_of_users = 14.719421610379321

benchmark_filename_list = ['train', 'test', 'test']

config = Config

dataset_name = 'amazon-book'

dataset_path = 'dataset/amazon-book'

entities = {ndarray:(24916,)}

entity2item = {dict:24915}

entity_field = 'entity_id'

entity_num = 24916

feat_name_list = ['inter_feat', 'kg_feat']

field2id_token = {dict: 6}
	'head_id' = {ndarray:(24916,)}
    'tail_id' = {ndarray:(24916,)}
    'relation_id' = {ndarray:(41,)}
    'user_id' = {ndarray:(70680,)}
    'item_id' = {ndarray:(24916,)}
    'entity_id' = {ndarray:(24916,)}
    
file_size_list = [652514, 193920, 193920]

inter_feat = {DataFrame:(1040354, 2)} ['user_id', 'item_id']

inter_num = 1040354

item_num = 24916

kg_feat = {DataFrame:(686516, 3)} ['head_id', 'relation_id', 'tail_id']

relation_num = 41

token_like_fields = ['entity_id', 'user_id', 'item_id', 'head_id', 'relation_id', 'tail_id']

user_num = 70680
```



# info(dataset)

```python
# 用户数量
The number of users: 70680
# 平均用户交互
Average actions of users: 14.719421610379321
# 物品数量
The number of items: 24916
# 平均物品交互
Average actions of items: 41.756130844872565
# 交互数
The number of inters: 1040354
# 数据集稀疏性
The sparsity of the dataset: 99.94092465341157%
# 保留域
Remain Fields: ['entity_id', 'user_id', 'item_id', 'head_id', 'relation_id', 'tail_id']
# 实体数量
The number of entities: 24916
# 关系数量
The number of entities: 24916
# 三元组数量
The number of triples: 686516
# 物品在 KG 中的映射
The number of items that have been linked to KG: 24915
```



# train/vaild/test data

```python
test_data ：<recbole.data.dataloader.general_dataloader.FullSortEvalDataLoader object at 0x7fea9ac69940>

config = Config

dataset = 

# 这三个有什么用？
uid2history_item = {ndarray:(70680)}
uid2items_num = {ndarray:(70680)}
uid2positive_item = {ndarray:(70680)}

uid_list = {Tensor：(70591,)}

-----------------------------------------------------------------------------

train_data ：<recbole.data.dataloader.knowledge_dataloader.KnowledgeBasedDataLoader object at 0x7fea9ac5ba00>

config = Config
dataset = 
```

# **KnowledgeBasedDataset**

```python
Bases: recbole.data.dataset.dataset.Dataset
除了基本的 test/train 文件外，额外附加了.kg 和 .link 文件
Entities 和 item_id 被重新映射，同时所有 entities 都被重新映射到三个连续的 ID sections
 -仅存在于交互数据中的虚拟实体
 -交互数据和 kg 三元组中都存在的实体
 -实体仅存在于 kg 三元组中
```

```python
head_entity_field: str
tail_entity_field: str
relation_field: str
entity_field: str
kg_feat: pandas.DataFrame,从 .kg 文件加载得到
item2entity: dict，从 .link 文件加载得到，将 item_id 映射到 entity
entity2item: dict，从 .link 文件加载得到，将 entity 映射到 item_id
```

# run_single_model

```python
def run_single_model(args):
    
    # config
    config = Config(
        model=SimKGCL,
        dataset=args.dataset,
        config_file_list=args.config_file_list,
        config_dict=args.config_dict
    )
    
    # 随机数种子
    init_seed(config['seed'], config['reproducibility'])
    
    # logger initialization
    # 初始化 logger
    init_logger(config)
    logger = getLogger()
    logger.info(config)
    
    # dataset filtering
    dataset = create_dataset(config)
    logger.info(dataset)
    
    # dataset splitting 数据集分割
    train_data, valid_data, test_data = data_preparation(config, dataset)
    
    # model loading and initialization
    model = SimKGCL(config, train_data.dataset).to(config['device'])
    logger.info(model)
    
    # trainer loading and initialization
    trainer = KCLTrainer(config, model)
    
    # model training 模型训练
    # best_valid_score NDCG@20
    # best_valid_result Recall@10/20/50 NDCG@10/20/50
    best_valid_score, best_valid_result = trainer.fit(
        train_data, valid_data, saved=True, show_progress=config['show_progress'])
    
    # model evaluation（现成的 不用重写
    test_result = trainer.evaluate(test_data, load_best_model=True,                                           show_progress=config['show_progress'])
    
    # 打印结果
    logger.info(set_color('best valid ', 'yellow') + f': {best_valid_result}')
    logger.info(set_color('test result', 'yellow') + f': {test_result}')
```















# Model

```python
class SimKGCL(KnowledgeRecommender):
    
    def __init__(self, config, dataset):
    super(SimKGCL, self).__init__(config, dataset)

    # load dataset info
    # dataset 的 inter_matrix 方法构 coo 交互矩阵（n_user*n_item）
    self.interaction_matrix = dataset.inter_matrix(form='coo').astype(np.float32)
    # 保存 user 数组（有重复）
    self._user = dataset.inter_feat[dataset.uid_field]  # train user array
    # 保存 item 数组（有重复）
    self._item = dataset.inter_feat[dataset.iid_field]  # train item array

    # load parameters info
    # 嵌入大小
    self.latent_dim = config['embedding_size']
    # GCN 层数
    self.n_layers = config['n_layers']
    # 正则化权重
    self.reg_weight = config['reg_weight']
    # 什么 pow？
    self.require_pow = config['require_pow']
    # 不存在
    self.layer_cl = config['layer_cl']
    # cl 率
    self.cl_rate = config['cl_rate']
    # 温度函数
    self.tau = config['tau']
    # kg 丢失率
    self.kg_drop_rate = config['kg_drop_rate']
    # ig 丢失率
    self.ig_drop_rate = config['ig_drop_rate']
    # mess 丢失率
    self.mess_drop_rate = config['mess_drop_rate']

    # define layers and loss
    # Embedding(70680, 64)
    self.user_embedding = torch.nn.Embedding(self.n_users, self.latent_dim)
    # Embedding(24916, 64)
    self.entity_embedding = torch.nn.Embedding(self.n_entities, self.latent_dim)
    # Embedding(42, 64)
    self.relation_embedding = torch.nn.Embedding(self.n_relations+1, self.latent_dim)
	# 丢弃层
    self.message_drop = torch.nn.Dropout(self.mess_drop_rate)
    # 节点丢弃
    self.node_drop = SparseDropout(self.ig_drop_rate)
	
    # Loss 函数
    self.mf_loss = BPRLoss()
    self.reg_loss = EmbLoss()

    # storage variables for full sort evaluation acceleration
    # 没用到
    self.restore_user_e = None
    self.restore_item_e = None

    # generate intermediate data
    # norm_adj_matrix: (n_user+n_item)*(n_user+n_item) (95596)*(95596)
    # user_item_matrix：n_user*n_item(70680,24916)
    self.norm_adj_matrix, self.user_item_matrix = self.get_norm_adj_mat()
    # (24916)*(24916)
    self.kg_graph = dataset.kg_graph(form="coo", value_field="relation_id")
    # kg 中 row 是头实体 686516
    self.all_hs = torch.LongTensor(self.kg_graph.row).to(self.device)
    # kg 中 col 是尾实体 686516
    self.all_ts = torch.LongTensor(self.kg_graph.col).to(self.device)
    # kg 中 data 是关系实体 686516
    self.all_rs = torch.LongTensor(self.kg_graph.data).to(self.device)

    # parameters initialization
    # 用 Xavier 初始化参数
    self.apply(xavier_uniform_initialization)
    self.other_parameter_name = ['restore_user_e', 'restore_item_e']
```

# info(model)

```python
SimKGCL(
  (user_embedding): Embedding(70680, 64)
  (entity_embedding): Embedding(24916, 64)
  (relation_embedding): Embedding(42, 64)
  (message_drop): Dropout(p=0.1, inplace=False)
  (node_drop): SparseDropout()
  (mf_loss): BPRLoss()
  (reg_loss): EmbLoss()
)

# 这啥
Trainable parameters: 6120832
```

# KCLTrainer

```python
class KCLTrainer(Trainer):
    
    def __init__(self, config, model):
        super(KCLTrainer, self).__init__(config, model)
		
        # None
        self.train_rec_step = config['train_rec_step']
        # None
        self.train_kg_step = config['train_kg_step']
        
	def fit(self, train_data, valid_data=None, verbose=True, saved=True, show_progress=False, callback_fn=None):
        # 没用到
        if saved and self.start_epoch >= self.epochs:
            self._save_checkpoint(-1)
		
        # 从训练集中提取 eval 集
        self.eval_collector.data_collect(train_data)
        
        # 训练过程
        for epoch_idx in range(self.start_epoch, self.epochs):
            
            # train 记录开始时间
            training_start_time = time()
            
            # 返回一个 epoch 的损失的值 (MF+CL)
            train_loss = self._train_epoch(train_data, epoch_idx,                                       show_progress=show_progress)
            
            # 保留对应 epoch 的 loss 值
            self.train_loss_dict[epoch_idx] = sum(train_loss) if isinstance(train_loss,                 tuple) else train_loss
            
            # train 记录结束时间
            training_end_time = time()
            
            # 训练损失
            train_loss_output = self._generate_train_loss_output(epoch_idx,                             training_start_time, training_end_time, train_loss)
            
            # verbose 是否展示结果，输出结果
            # INFO  epoch 0 training [time: 154938.80s, train_loss1: 315072.4861,                         train_loss2: 1797185.3271]
            if verbose:
                self.logger.info(train_loss_output)
            # 把这个什么添加到 tensorboard
            self._add_train_loss_to_tensorboard(epoch_idx, train_loss)
            
            
            # 如果 eval_step 小于等于零，或者 valid_data 为假，就执行
            if self.eval_step <= 0 or not valid_data:
                if saved:
                    self._save_checkpoint(epoch_idx)
                    update_output = set_color('Saving current', 'blue') + ': %s' %                               self.saved_model_file
                    if verbose:
                        self.logger.info(update_output)
                continue
                
            # 判断当前轮数是否是验证的步数的倍数
            if (epoch_idx + 1) % self.eval_step == 0:
                # 验证起始时间
                valid_start_time = time()
                
                # 验证分数、验证结果
                valid_score, valid_result = self._valid_epoch(valid_data,                                   show_progress=show_progress)
                
                # 停止标记
                stop_flag = False
                # 更新标记
                update_flag = True
                
                # 判断 epoch_idx 是否大于等于 0
                if epoch_idx >= 0:
                    # early_stopping 操作；10个 step 判断是否停止训练
                    self.best_valid_score, self.cur_step, stop_flag, update_flag =                               early_stopping(
                        valid_score,
                        self.best_valid_score,
                        self.cur_step,
                        max_step=self.stopping_step,
                        bigger=self.valid_metric_bigger
                    )
                
                # 验证结束时间
                valid_end_time = time()
                
                # 验证分数输出 str
                valid_score_output = (set_color("epoch %d evaluating", 'green') + " [" +                     set_color("time", 'blue') + ": %.2fs, " + set_color("valid_score", 'blue') +                 ": %f]") % \ (epoch_idx, valid_end_time - valid_start_time, valid_score)
                # 验证结果输出 str
                valid_result_output = set_color('valid result', 'blue') + ': \n' +                           dict2str(valid_result)
                # 展示结果
                if verbose:
                    self.logger.info(valid_score_output)
                    self.logger.info(valid_result_output)
                self.tensorboard.add_scalar('Vaild_score', valid_score, epoch_idx)
                
                # 更新
                if update_flag:
                    if saved:
                        # 保存
                        self._save_checkpoint(epoch_idx)
                        # 更新结果 也没保存啊？
                        update_output = set_color('Saving current best', 'blue') + ': %s' %                         self.saved_model_file
                        if verbose:
                            # 终端输出结果
                            self.logger.info(update_output)
                    
                    # 替换最好结果
                    self.best_valid_result = valid_result
                
                # 如果训练好了
                if stop_flag:
                    stop_output = 'Finished training, best eval result in epoch %d' % \
                                  (epoch_idx - self.cur_step * self.eval_step)
                    if verbose:
                        self.logger.info(stop_output)
                    break
        
        # 保存对应超参
        self._add_hparam_to_tensorboard(self.best_valid_score)            
```

# _train_epoch

```python
def _train_epoch(self, train_data, epoch_idx, loss_func=None, show_progress=False):
    
    # 将模型调整为训练模式
    self.model.train()
    # loss 计算函数为 self.model.calculate_loss 中的这个函数
    loss_func = loss_func or self.model.calculate_loss
    # 统计总 loss
    total_loss = None
    # 迭代数据，如果需要显示进度条，那么就显示
    iter_data = (
            tqdm(
                # 可迭代对象
                train_data,
                # 长度
                total=len(train_data),
                # 进度条宽度
                ncols=100,
                # 输出颜色
                desc=set_color(f"Train {epoch_idx:>5}", 'pink'),
            ) if show_progress else train_data
        )
    # batch_idx = 0；interaction: 2048
    # The batch_size of interaction: 2048
    # user_id, torch.Size([2048]), cpu, torch.int64
    # item_id, torch.Size([2048]), cpu, torch.int64
    # neg_item_id, torch.Size([2048]), cpu, torch.int64

    for batch_idx, interaction in enumerate(iter_data):
        # 交互数据
        interaction = interaction.to(self.device)
        # 梯度归零
        self.optimizer.zero_grad()
        # 计算 loss
        losses = loss_func(interaction)
        
        # 判断 loss 是否是 tuple
        if isinstance(losses, tuple):
            # 损失求和
        	loss = sum(losses)
            # Tensor 转 tuple
            loss_tuple = tuple(per_loss.item() for per_loss in losses)
            # 将 loss_tuple 中的损失值累加到 total_loss 中
            total_loss = loss_tuple if total_loss is None else tuple(map(sum, 	                         zip(total_loss, loss_tuple)))
		else:
            loss = losses
            total_loss = losses.item() if total_loss is None else total_loss + losses.item()
        # 检查 loss 是否为空值
		self._check_nan(loss)
        # 反向传播
        loss.backward()
        # 判断
		if self.clip_grad_norm:
        	clip_grad_norm_(self.model.parameters(), **self.clip_grad_norm)
        # 梯度更新
        self.optimizer.step()
```

# calculate_loss

```python
def calculate_loss(self, interaction):
    
	# clear the storage variable when training
    if self.restore_user_e is not None or self.restore_item_e is not None:
        self.restore_user_e, self.restore_item_e = None, None
    
    # 包含 user 索引的 tensor
    user = interaction[self.USER_ID]
    # 正样本
    pos_item = interaction[self.ITEM_ID]
    # 负样本
    neg_item = interaction[self.NEG_ITEM_ID]
    
    # 从 forward 获取四个嵌入
    user_all_embeddings, item_all_embeddings, \
        user_kg_emb, item_kg_emb = self.forward(cl=True, Drop=True)
    
    # 根据 batch 中得 user_index 提取对应 user 得 emb
    # (2048, 64)
    u_embeddings = user_all_embeddings[user]
    # 正样本 (2048, 64)
    pos_embeddings = item_all_embeddings[pos_item]
    # 负样本 (2048, 64)
	neg_embeddings = item_all_embeddings[neg_item]
    
    # calculate reg loss / 计算正则化损失 user/pos_item/neg_item 是 index
    u_ego_embeddings = self.user_embedding(user)
    pos_ego_embeddings = self.entity_embedding(pos_item)
    neg_ego_embeddings = self.entity_embedding(neg_item)
    
    reg_loss = (torch.norm(u_ego_embeddings, p=2) + torch.norm(pos_ego_embeddings, p=2) \
                    + torch.norm(neg_ego_embeddings, p=2)) * self.reg_weight
    
    # 计算 BPR Loss
    # 正样本分数
    pos_scores = torch.mul(u_embeddings, pos_embeddings).sum(dim=1)
    # 负样本分数
    neg_scores = torch.mul(u_embeddings, neg_embeddings).sum(dim=1)
    # 损失
    mf_loss = -torch.log(1e-10 + torch.sigmoid(pos_scores - neg_scores)).sum()
    # 损失和
    mf_loss = mf_loss + reg_loss
    
    # 计算自监督损失
    ssl_loss2 = self.calculate_ssl_loss(user, pos_item, user_all_embeddings, user_kg_emb,                   item_all_embeddings, item_kg_emb)
    
    # 返回两种损失
    return mf_loss, ssl_loss2
```

# forward

```python
def forward(self, cl=False, Drop=False):
    
    # 将 user 和 item 的 embedding 按行组合
    ego_embeddings = self.get_ego_embeddings()
    
    # 所有 ig 的嵌入
    all_ig_embeddings = []
    # 所有 kg 的嵌入，why？
    all_kg_embeddings = [ego_embeddings]
    
    # GCN 卷积次数
    for k in range(self.n_layers):
        # (95596,95596)x(95596,64) = (95596,64) AX = D^(-1/2)AD^(-1/2)X
        ig_embeddings = torch.sparse.mm(self.norm_adj_matrix, 		                                 ego_embeddings[:self.n_users+self.n_items])
        
        # 返回拼接的 user/entity (95596,64)
        kg_embeddings = self.kg_forward(all_kg_embeddings[-1], Drop=Drop)
        
        # 为什么又要重新赋值
    	ego_embeddings = kg_embeddings
        
        # 将 KG 和 IG 的 emb 融合
        ego_embeddings[:self.n_items+self.n_users] += ig_embeddings
        
        # 往 KG 和 IG 列表里添加第 l 层的 emb
    	all_ig_embeddings.append(ego_embeddings)
        all_kg_embeddings.append(kg_embeddings)
    
    # final_emb 是 all_ig_emb 的堆叠
    final_embeddings = torch.stack(all_ig_embeddings, dim=1)
    # 在 dim = 1 的维度上求平均
    final_embeddings = torch.mean(final_embeddings, dim=1)
    # 提取 user 和 item 的 emb
    final_embeddings = final_embeddings[:self.n_users + self.n_items]
    # 又分开 user 和 item
    user_all_embeddings, item_all_embeddings = torch.split(final_embeddings, [self.n_users,     self.n_items])
    
    # 我传我自己 (list: 4) (95596, 64)
    all_kg_embeddings = all_kg_embeddings
    # 堆叠 (95596, 4, 64)
    final_kg_embeddings = torch.stack(all_kg_embeddings, dim=1)
    # 求平均 (95596, 64)
    final_kg_embeddings = torch.mean(final_kg_embeddings, dim=1)
    # 提取 user 和 item 的 emb
    final_kg_embeddings = final_kg_embeddings[:self.n_users+self.n_items]
    # 分离
    user_kg_embeddings, item_kg_embeddings = torch.split(final_kg_embeddings, [self.n_users,     self.n_items])
    
    # 如果 cl 就返回 IG 和 KG 得 emb，否则不要 KG
    if cl:
    	return user_all_embeddings, item_all_embeddings, user_kg_embeddings,                         item_kg_embeddings
    return user_all_embeddings, item_all_embeddings
    
    
```

# kg_forward

```python
def kg_forward(self, ego_embeddings, Drop=False):
    # 把组合的 ego_embeddings 又分开
    user_emb, entity_emb = torch.split(ego_embeddings, [self.n_users, self.n_entities])
    # 丢弃 KG 中三元组的边
    if Drop and self.kg_drop_rate > 0.0:
        # 获得丢弃的 h, t, r 实体
        all_h, all_t, all_r = self.edge_sampling(self.all_hs, self.all_ts, self.all_rs, 1-           self.kg_drop_rate)
    # 丢弃 IG 中的边
    if Drop and self.ig_drop_rate > 0.0:
        # 获得 Dropout 的 IM
        inter_matrix = self.node_drop(self.user_item_matrix)
    else:
        inter_matrix = self.user_item_matrix
    
    # 关系嵌入就是关系嵌入 
    relation_emb = self.relation_embedding.weight
    
    # entity 和 user 嵌入
    entity_emb, user_emb = self.kg_agg(entity_emb, user_emb, relation_emb, all_h, all_t,         all_r, inter_matrix)
    
    # Dropout 操作
    if Drop and self.mess_drop_rate > 0.0:
    	entity_emb = self.message_drop(entity_emb)
    	user_emb = self.message_drop(user_emb)
    
    # 归一化操作
    entity_emb = F.normalize(entity_emb)
    user_emb = F.normalize(user_emb)
    
    # 按行拼接返回
    return torch.cat([user_emb, entity_emb], dim=0)
```

# kg_agg

```python
def kg_agg(self, entity_emb, user_emb, relation_emb, all_h, all_t, all_r, inter_matrix, attention=True):
    
    # scatter_softmax: 用于在图上执行 softmax 操作，通常用于对图节点的特征进行归一化
	# scatter_mean: 用于在图上执行均值操作，通常用于聚合节点的邻居特征，计算节点的平均特征
    from torch_scatter import scatter_softmax, scatter_mean
	
    # entities 的数量是 entity_emb 的行数
	n_entities = entity_emb.shape[0]
    # all_r 是三元组的索引，这么做是为了提取对应 emb
    edge_relation_emb = relation_emb[all_r]
    # 邻居关系嵌入，邻居尾实体 * 边嵌入
    neigh_relation_emb = (entity_emb[all_t] * edge_relation_emb)
    
    # 如果使用 attention 机制
    if attention:
        
        # Tensor: (617864, 1)
        neigh_relation_emb_weight = self.calculate_sim_hrt(
                entity_emb[all_h], entity_emb[all_t], edge_relation_emb)
        
        # Tensor: (617864, 64) 扩展到 64 维
        neigh_relation_emb_weight = neigh_relation_emb_weight.expand(
                neigh_relation_emb.shape[0], neigh_relation_emb.shape[1])
        
        # softmax
        # src (Tensor) – The source tensor
        # index (LongTensor) – The indices of elements to scatter
        # dim (int, optional) – The axis along which to index. (default: -1)
        neigh_relation_emb_weight = scatter_softmax(
                neigh_relation_emb_weight, index=all_h, dim=0)
        
        # 逐元素相乘
        # Tensor: (617864, 64) mul Tensor: (617864, 64)
        neigh_relation_emb = torch.mul(
                neigh_relation_emb_weight, neigh_relation_emb)
    
    # 这啥？Tensor: (24916, 64)
    entity_agg = scatter_mean(
            src=neigh_relation_emb, index=all_h, dim_size=n_entities, dim=0)
    
    # (70680, 24916) * (24916, 64) -> (70680, 64)
    user_agg = torch.sparse.mm(inter_matrix, entity_emb[:self.n_items])
    
    # user 和 relation 相似度分数 
    # (70680, 64) * (64, 42) -> (70680, 42) 表示用户对不同关系的概率分布
    score = torch.mm(user_emb, relation_emb.t())
    
    # 关系的加权嵌入 与 user 相乘，更新 user_agg
    user_agg = user_agg + (torch.mm(score, relation_emb)) * user_agg
    
    # 返回这两个东西
    return entity_agg, user_agg
```

# calculate_sim_hrt

```python
def calculate_sim_hrt(self, entity_emb_head, entity_emb_tail, relation_emb):
    
    # tail_emb 和 real_emb 逐元素相乘
    # Tensor: (617864, 64)
    tail_relation_emb = entity_emb_tail * relation_emb
    # L2 归一化操作
    # Tensor: (617864, 1)
    tail_relation_emb = tail_relation_emb.norm(dim=1, p=2, keepdim=True)
    # head_emb 和 real_emb 逐元素相乘
    # Tensor: (617864, 64)
    head_relation_emb = entity_emb_head * relation_emb
    # L2 归一化操作
    # Tensor: (617864, 1)
    head_relation_emb = head_relation_emb.norm(dim=1, p=2, keepdim=True)
    
    # 注意力权重 (617864, 1)
    att_weights = torch.matmul(
        # (617864, 1, 1), (617864, 1, 1)
        head_relation_emb.unsqueeze(dim=1), tail_relation_emb.unsqueeze(dim=2)
    ).squeeze(dim=-1)
    # 每个元素求平方
    att_weights = att_weights**2
    
    # 返回权重
    return att_weights
```

# calculate_ssl_loss

```python
# user: user index 
# item: 正样本 index
# user_embeddings_v1: 全部 user_emb (70680, 64)
# user_embeddings_v2: user_kg_emb (70680, 64)
# item_embeddings_v1: 全部 item_emb (24916, 64)
# item_embeddings_v2: item_kg_emb (24916, 64)
def calculate_ssl_loss(self, user, item, user_embeddings_v1,
                           user_embeddings_v2, item_embeddings_v1, item_embeddings_v2):
    
    # 首先使用 unique 函数提取唯一 user 标识，然后从提取对应 emb 之后归一化
    # (1931, 64)
    norm_user_v1 = F.normalize(user_embeddings_v1[torch.unique(user)])
    norm_user_v2 = F.normalize(user_embeddings_v2[torch.unique(user)])
    # (1764, 64)
    norm_item_v1 = F.normalize(item_embeddings_v1[torch.unique(item)])
    norm_item_v2 = F.normalize(item_embeddings_v2[torch.unique(item)])
    
    # 计算 user 在 IG 和 KG 之间得相似度 (1931, )
    user_pos_score = torch.mul(norm_user_v1, norm_user_v2).sum(dim=1)
    # (1931, 1931)
    user_ttl_score = torch.matmul(norm_user_v1, norm_user_v2.t())
    # 对比损失的分子
    user_pos_score = torch.exp(user_pos_score / self.tau)
    # 对比损失的分母
    user_ttl_score = torch.exp(user_ttl_score / self.tau).sum(dim=1)
    # user 的总 cl 损失
    user_ssl_loss = -torch.log(user_pos_score / user_ttl_score).sum()
    
    # 同理 item 的 cl 损失
    item_pos_score = torch.mul(norm_item_v1, norm_item_v2).sum(dim=1)
    item_ttl_score = torch.matmul(norm_item_v1, norm_item_v2.t())
    item_pos_score = torch.exp(item_pos_score / self.tau)
    item_ttl_score = torch.exp(item_ttl_score / self.tau).sum(dim=1)
    item_ssl_loss = -torch.log(item_pos_score / item_ttl_score).sum()
    
    # 求和 user 和 item 损失
    ssl_loss = user_ssl_loss + item_ssl_loss
    
    # 返回带权重的 cl 损失
    return ssl_loss * self.cl_rate
    
```

# _valid_epoch

```python
def _valid_epoch(self, valid_data, show_progress=False):
    
    # 验证结果
    # recall@10/20/50 ndcg@10/20/50
    valid_result = self.evaluate(valid_data, load_best_model=False,                             show_progress=show_progress)
    
    # 验证分数来确定最佳模型 这里选择的是 ndcg@20
    valid_score = calculate_valid_score(valid_result, self.valid_metric)
    return valid_score, valid_result
```

