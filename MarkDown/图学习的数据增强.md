# 1 INTRODUCTION

GNN 在现实世界图上的性能往往很脆弱，主要是因为它们无法处理以下挑战

1. 当真实标签极其稀缺时，DGL模型可能很容易过度拟合并且难以泛化，从而失去解决各种下游DGL任务的功效
2. 现实世界的图通常是从复杂的交互系统中提取的，这些系统不可避免地包含冗余、错误或缺失的特征和连接

为了提高训练数据的充分性和质量，数据增强被提出作为一种有效的工具，通过稍微修改现有数据实例或从现有数据实例生成合成实例来增强给定的输入数据

# 2 PRELIMINARIES

## 2.1 Notations and Definitions

略

## 2.2 Graph Neural Networks

略

## 2.3 Deep Graph Learning Tasks

- semi-supervised node classification
- link prediction
- graph classification



# 3. TECHNIQUES OF GRAPH DATA AUGMENTATION

图数据增强（GraphDA）的目标是找到一个变换函数生成增强图，可以丰富或增强给定图中保留的信息。就学习过程中参数 θ 是否可以更新而言，大多数（如果不是全部）GraphDA 方法可以分为：不可学习和可学习方法

总的来说，由于 GraphDA 的最终目标是提高 GNN 模型在下游学习任务上的性能，因此我们需要在学习过程中将它们一起考虑

大多数（如果不是全部）可学习的 GraphDA 方法可以分为三类

- decoupled training
- joint training
- bi-level optimization

<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017103207314.png" alt="image-20231017103207314" style="zoom:80%;" />

## 3.1 Structure-oriented Augmentations

给定输入图 G = (A; X)，面向结构的 GraphDA 操作侧重于增强输入图的邻接矩阵 A。我们将其中有代表性的总结如下

**Edge Perturbation**

扰动给定的图结构，例如随机添加或删除边，是不同 DGL 任务中广泛采用的 GraphDA 方法，数学上的边缘扰动保持原始节点顺序并重写给定邻接矩阵中的部分条目，可以定义如下

![image-20231017104301264](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017104301264.png)

**Graph Rewiring**

与边缘扰动共享相同的基本操作，通过重新构建边缘来提高输入图的效用。图重新构建边缘通常由下游任务的学习目标引导，而不是通过随机添加/删除边来扰乱输入图结构，并且通过特定模块学习或预测损坏矩阵 C

**Graph Diffusion**

图扩散通过利用输入图的全局结构知识来生成增强图。在某些情况下，它也被认为是一种 Graph Rewiring[98]。图扩散通过使用计算的权重将节点与其间接连接的邻居连接起来，将全局拓扑信息注入到给定的图结构中

![image-20231017111412027](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017111412027.png)

**Graph Sampling**

增强图是通过采样器 Sample(G) 获得的，采样器可以是基于顶点的采样 [50]、基于边的采样 [147]、基于遍历的采样 [84] 和其他高级方法，例如 Metropolis-Hastings 采样 [81]

![image-20231017111924285](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017111924285.png)

一般来说，图采样的目标是从输入图中找到增强图实例，这些实例通过保留节点的一部分及其底层链接

**Node Dropping**

节点丢弃也称为节点屏蔽

![image-20231017112033343](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017112033343.png)

**Node Insertion**

节点插入通常用于通过插入虚拟节点来改善输入图上的消息传递或连接性

![image-20231017143619557](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017143619557.png)

由于节点插入还需要在新图中添加额外的边，因此该 GraphDA 操作与 graph rewriting 高度相关。请注意，对于属性图，还需要初始化相应的节点特征，例如，使用所有连接节点特征的平均值

**Graph Generation**

大多数图形生成方法都有从观察到的图中自动学习

![image-20231017143827219](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017143827219.png)

例如图粗化（graph coarsening）[8]和图压缩（graph condensation）[55]，其目标是从初始大图生成新图，也可以归类为这种增强操作



## 3.2 Feature-oriented Augmentations

回顾面向特征的 GraphDA 技术。一般来说，给定输入图 G = (A; X)，面向特征的 GraphDA 操作侧重于对节点特征矩阵 X 执行变换。值得注意的是，我们还认为那些对潜在特征表示 H 执行增强的方法是面向特征的增强方法

**Feature Corruption**

该 GraphDA 方法旨在向原始节点特征 [27] 或学习到的特征表示 [123] 添加噪声

![image-20231017145936544](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017145936544.png)

特征噪声可以随机添加[99]，也可以通过对抗训练方式学习[27；123]

**Feature Shuffling**

通过切换特征矩阵中的行和列来随机改变上下文信息，输入特征矩阵 X 被破坏以产生增强。该方法可以表述为

![image-20231017151602268](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017151602268.png)

**Feature Masking**

特征屏蔽的核心操作是将节点特征矩阵X中的部分条目设置为0，可以表示为

![image-20231017151738869](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017151738869.png)

掩蔽矩阵 M 通常由伯努利分布生成

**Feature Addition**

一种直接的方法是将邻近/拓扑信息（例如，节点索引或节点属性）编码到特征向量中并与原始节点特征连接

**Feature Rewriting**

考虑到给定的节点特征通常是有噪声且不完整的，恢复干净且完整的节点特征可以直接提高DGL模型的性能

![image-20231017152420291](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017152420291.png)

其中 α 和 β 是两个控制参数，bi 是以启发式或可学习的方式计算的特征向量。例如，Wang等人提出特征替换[108]，用其邻居的特征重写节点特征。 Xu等人[121]应用基于梯度下降的优化器将节点特征重写为参数

**Feature Mixing**

根据输入图中节点的特征，可以使用特征混合来获得合成节点的节点特征

![image-20231017152735776](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017152735776.png)

**Feature Propagation**

特征传播基于图扩散，沿着图结构传播节点特征。它是一种插值方法，也被广泛用于增强输入图的节点特征

![image-20231017152957511](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017152957511.png)

## 3.3 Label-oriented Augmentations



# 4. GRAPH DATA AUGMENTATION FOR LOW-RESOURCE GRAPH LEARNING

真实标签的短缺一直是一个长期存在且臭名昭著的问题。为了推进低资源图学习的研究，GraphDA 得到了积极的研究并显示出有希望的结果

## 4.1 Graph Self-Supervised Learning

**Graph Generative Modeling**

图生成建模方法通过

edge perturbation

feature masking

node dropping

对输入图执行数据增强，然后通过从增强的图中重建特征或/和结构信息来学习节点表示

<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017160608160.png" alt="image-20231017160608160" style="zoom:80%;" />

例如，去噪链接重建[45]\(Denoising Link Reconstruction)随机丢弃现有边以获得扰动图，并尝试使用基于成对相似性的解码器恢复丢弃的连接



**Graph Contrastive Learning**

数据增强也被广泛用于图对比学习（GCL）

<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017160624512.png" alt="image-20231017160624512" style="zoom:80%;" />

DGI 应用 feature shuffling 和 edge perturbation 来获得图的负对，然后提出一个对比目标来最大化节点嵌入和全局摘要嵌入之间的互信息，GCL 的另一种流行的 GraphDA 方法是 graph sampling。例如，GCC [84]建议将子图采样为对比实例来预训练图编码器，该编码器可以通过冻结或完全微调策略用于不同的下游任务 

值得注意的是，许多 GCL 方法通常利用增强策略的组合。例如，MVGRL [38] 首先通过图扩散来增强输入图。然后，通过子图采样生成两个图视图，并且模型学习将两个视图中的节点表示与全局表示进行对比

![image-20231017162958705](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017162958705.png)



## 4.2 Graph Semi-Supervised Learning

![image-20231017163828830](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231017163828830.png)

