# 感知机

最初的感知机是解决二分类问题，给这类分类问题提供一种统一的模板

![image-20230411100844126](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230411100844126.png)

![image-20230411101600701](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230411101600701.png)

- 线性函数：二分类的那条分界线
- 激活函数：做判断并输出结果
- 缺陷：没有办法解决异或问题
- 解决方法：多层感知机/升维



# 神经网络

换激活函数从感知机到神经网络的跃迁，从是非问题变成了概率问题

内核是神经网络会建立一个自己的标准，通过不断训练，达到一个满意的结果



# 损失函数

神经网络对标准的理解和人的理解之间差距的定量表达

两个概率模型是怎么比较的



## 最小二乘法

![image-20230411104954324](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230411104954324.png)

 

这里$$x_i$$是输入数据（标注数据），也可以理解为人类所做出的判断，$$y_i$$是模型所做出的预测，比如sigmoid激活函数所给出的0-1之间的值，表示模型认为是猫的概率有多大

单个数据
$$
|x_i - y_i|
$$
整体数据
$$
\sum_{i=1}^{n}|x_{i}-y_{i}|
$$
为了获得最优模型，这个值应该最小
$$
\operatorname*{min}\sum_{i=1}^{n}|x_{i}-y_{i}|
$$
进一步优化，使得全程可导，将绝对值换成平方，虽然改变了值，但是对模型来说效果是一样的
$$
\operatorname*{min}\sum_{i=1}^{n}(x_{i}-y_{i})^2
$$




## 极大似然估计

最大似然估计得出的模型是根据已有数据推理计算出最有可能的模型，但并不一定是最真实的模型，wb是可选的（理论世界），对于 不同的（w，b）判断一张图片是猫的概率不同，但是图片是否是猫是确定的（真是世界），所以把所有判断对的概率相乘，取最大值对应的wb作为模型的参数。

![image-20230411110648579](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230411110648579.png)

$$y_i$$的结果依赖$$W,b$$
$$
l o g\left(\prod_{i=1}^{n}y_{i}{}^{x_{i}}(1-y_{i})^{1-x_{i}}\right)
$$
$$
=\sum_{i=1}^{n}l o g(y_{i}^{x_{i}}(1-y_{i})^{1-x_{i}}) 
$$
$$
=\sum_{i=1}^{n}(x_{i}\cdot l o g\,y_{i}\,+(1-x_{i})\cdot l o g(1-y_{i}))
$$


$$
\begin{array}{c}{{m a x\ (\sum_{i=1}^{n}(x_{i}\cdot l o g\,y_{i}+(1-x_{i})\cdot l o g(1-y_{i})))}}\\ {{m i n-(\sum_{i=1}^{n}(x_{i}\cdot l o g\,y_{i}+(1-x_{i})\cdot l o g(1-y_{i})))}}\end{array}
$$
[损失函数的交叉熵与极大似然估计推导 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/458745814)

## 交叉熵



![image-20230415145308704](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230415145308704.png)







# Under-Samping

![image-20230412212533524](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230412212533524.png)

我们要想用这样的数据去建模显然是存在问题的。尤其是在我们更关心少数类的问题的时候数据分类不均衡会更加的突出，例如，信用卡诈骗、病例分析等。在这样的数据分布的情况下，运用机器学习算法的预测模型可能会无法做出准确的预测，最后的模型显然是趋向于预测多数集的，少数集可能会被当做噪点或被忽视，相比多数集，少数集被错分的可能性很大。从本质上讲，机器学习算法就是从大量的数据集中通过计算得到某些经验，进而判定某些数据的正常与否。但是，不均衡数据集，显然少数类的数量太少，模型会更倾向于多数集。


常用的下采样方法
**解决数据分布不均衡的下采样的目的就从多数集中选出一部分数据与少数集重新组合成一个新的数据集。那么如何在多数集中选出这样的数据呢？**

1. 随机下采样
   随机欠采样的思想同样比较简单，就是从多数类样本中随机选取一些剔除掉。这种方法的缺点是被剔除的样本可能包含着一些重要信息，致使学习出来的模型效果不好。

2. EasyEnsemble 和 BalanceCascade
   EasyEnsemble和BalanceCascade采用集成学习机制来处理传统随机欠采样中的信息丢失问题。

  EasyEnsemble将多数类样本随机划分成n个子集，每个子集的数量等于少数类样本的数量，这相当于欠采样。接着将每个子集与少数类样本结合起来分别训练一个模型，最后将n个模型集成，这样虽然每个子集的样本少于总体样本，但集成后总信息量并不减少。

  如果说EasyEnsemble是基于无监督的方式从多数类样本中生成子集进行欠采样，那么BalanceCascade则是采用了有监督结合Boosting的方式（Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造一个预测函数系列,然后以一定的方式将他们组合成一个预测函数）。在第n轮训练中，将从多数类样本中抽样得来的子集与少数类样本结合起来训练一个基学习器H，训练完后多数类中能被H正确分类的样本会被剔除。在接下来的第n+1轮中，从被剔除后的多数类样本中产生子集用于与少数类样本结合起来训练，最后将不同的基学习器集成起来。BalanceCascade的有监督表现在每一轮的基学习器起到了在多数类中选择样本的作用，而其Boosting特点则体现在每一轮丢弃被正确分类的样本，进而后续基学习器会更注重那些之前分类错误的样本。

3. NearMiss
   NearMiss本质上是一种原型选择(prototype selection)方法，即从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。NearMiss采用一些启发式的规则来选择样本，根据规则的不同可分为3类：

  NearMiss-1：选择到最近的K个少数类样本平均距离最近的多数类样本
  NearMiss-2：选择到最远的K个少数类样本平均距离最近的多数类样本
  NearMiss-3：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围
  NearMiss-1和NearMiss-2的计算开销很大，因为需要计算每个多类别样本的K近邻点。另外，NearMiss-1易受离群点的影响，如下面第二幅图中合理的情况是处于边界附近的多数类样本会被选中，然而由于右下方一些少数类离群点的存在，其附近的多数类样本就被选择了。相比之下NearMiss-2和NearMiss-3不易产生这方面的问题。



# 梯度下降

输出
$$
y=f(x,\theta)
$$
损失函数
$$
L(f(x_{i},\theta),y_{i})
$$
求梯度
$$
\nabla _\theta L(f(x_{i},\theta),y_{i})
$$

$$
g\,=\frac 1  N\nabla_{\theta}\,\,y\,\,\,L(f(x_{i},\theta),y_{i})
$$
更新参数
$$
\theta \gets \theta - \varepsilon g
$$




**Adam算法 = 传统 + 动量 + 自调节lr**





# 反向传播

![image-20230419204059613](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419204059613.png)


$$
\frac {\partial{L}} {\partial{w}} \,\,\,\,\,\,\,\,\,\,\,\,\,\,w \gets w - \varepsilon \frac {\partial{L}} {\partial{w}}
$$

$$
\frac {\partial{L}} {\partial{b}} \,\,\,\,\,\,\,\,\,\,\,\,\,\,b \gets b - \varepsilon \frac {\partial{L}} {\partial{b}}
$$
**链式求导**

![image-20230419204714257](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419204714257.png)

![image-20230419205007441](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419205007441.png)

**多层网络**

![image-20230419205120756](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419205120756.png)

**计算图**

![image-20230419205246166](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419205246166.png)

![image-20230419205803884](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419205803884.png)

![image-20230419205843101](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419205843101.png)

![image-20230419210000069](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419210000069.png)

![image-20230419210109311](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419210109311.png)



![image-20230419233800448](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419233800448.png)



# 激活函数

![image-20230419210606569](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230419210606569.png)

```python
# 定义神经网络结构和权重
import numpy as np

# 网络结构
input_size = 2
hidden_size = 4
output_size = 1

# 随机初始化权重
W1 = np.random.randn(input_size, hidden_size)
W2 = np.random.randn(hidden_size, output_size)

# 前向传播函数
def forward(X):
    # 第一层
    z1 = np.dot(X, W1)
    a1 = np.tanh(z1)

    # 输出层
    z2 = np.dot(a1, W2)
    a2 = np.sigmoid(z2)

    return a2, (a1, z1, z2)

# 反向传播函数
def backward(X, Y, cache):
    a1, z1, z2 = cache

    # 计算输出层误差
    delta2 = a2 - Y
    dW2 = np.dot(a1.T, delta2)

    # 计算第一层误差
    delta1 = np.dot(delta2, W2.T) * (1 - np.power(a1, 2))
    dW1 = np.dot(X.T, delta1)

    return dW1, dW2

# 更新权重函数
def update_weights(W1, W2, dW1, dW2, learning_rate):
    W1 -= learning_rate * dW1
    W2 -= learning_rate * dW2
    return W1, W2

# 训练函数
def train(X, Y, num_epochs, learning_rate):
    for i in range(num_epochs):
        # 前向传播
        a2, cache = forward(X)

        # 反向传播
        dW1, dW2 = backward(X, Y, cache)

        # 更新权重
        W1, W2 = update_weights(W1, W2, dW1, dW2, learning_rate)

        # 打印损失
        loss = np.mean(np.square(a2 - Y))
        print('Epoch:', i, 'Loss:', loss)

# 生成数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
Y = np.array([[0], [1], [1], [0]])

# 训练网络
train(X, Y, 1000, 0.1)
```

二分类交叉熵（Binary Cross-Entropy）损失函数是用于二分类问题的一种常见的损失函数。对于一个二分类问题，假设模型的预测输出为 a，真实输出为 y，则二分类交叉熵损失函数的定义如下：

```python
L(a, y) = -[y * log(a) + (1 - y) * log(1 - a)]
```

其中，log表示自然对数。当 y=1 时，第一项起作用，当 y=0 时，第二项起作用。该损失函数可以看作是最大似然估计的负对数似然函数。

在批量训练过程中，通常使用所有训练样本的平均损失作为模型的总损失：

L = 1/N * Σ L(a, y)

其中，N为训练样本数量。

在反向传播过程中，需要计算损失函数对输出层的导数，其表达式为：

dL/da = -[y/a - (1 - y)/(1 - a)]

**巧合的是当sigmoid为激活函数时**

sigmoid(a)求导之后的结果等于 a(1-a) 

对于二元交叉熵损失函数，该导数可以简化为：

dL/da = (a - y)

