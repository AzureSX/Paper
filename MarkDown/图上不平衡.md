# Abstract

图代表了无数现实场景中普遍存在的互连结构

有效的图分析（例如图学习方法）使用户能够从图数据中获得深刻的见解，支撑包括节点分类和链接预测在内的各种任务，然而，这些方法经常遇到数据不平衡的问题，这是图数据中的一个常见问题，其中某些部分拥有丰富的数据，而其他部分则稀缺，从而导致学习结果有偏差

# 1 Introduction

**Graph Representation Learning**

图表示学习旨在将图的结构（例如节点、边或图）嵌入到低维空间中，同时保留其结构信息

**Imbalance phenomenon**

模型往往在拥有充足数据的高资源群体上得到良好的训练，而在数据有限的低资源群体上表现不佳，导致类别边界不清晰

**Imbalanced learning on graphs**

图数据与它们的不同之处在于样本本质上是非独立同分布的。并且多种多样，具有不同的结构方面

<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016170905101.png" alt="image-20231016170905101" style="zoom:80%;" />

# 2 Background

## 2.1 Graphs and Graph Representation Learning

- homogeneous graph：不考虑节点和边类型的图
- heterogeneous graph：Heterogeneous Information Network(HIN) 具有不同节点或边类型的图
  - bipartite graph：二分图要求只存在两种类型的节点，边连接不同类型的节点
  - knowledge graph：是由节点之间各种类型的边组成的图，这些边也分别称为关系和实体

## 2.2 Imbalanced Learning on Graphs

**Imbalanced learning**

不平衡学习的传统方法主要集中于开发学习算法来处理不平衡类别

**Imbalanced learning on graphs**

图上的不平衡学习很复杂。除了类不平衡之外，图结构本身也是不平衡的一个重要来源，这可能会导致不同组之间的性能差异

![image-20231016191627228](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016191627228.png)

# 3 OVERVIEW OF TAXONOMIES

**Taxonomy of Problems**

图不平衡可能来自两个关键来源：类和结构,它们通常作为学习模型的关键输入一方面，对于图中的类，标记数据可能在类之间分布不均匀，导致节点分类等任务的结果不平衡另一方面，独特的图结构可能会引入图学习模型不平衡的另一个来源，例如，节点的邻域丰度可能会导致暴露信息的差异，这可能会相应地进一步扭曲学习模型的输出

<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016192100263.png" alt="image-20231016192100263" style="zoom:80%;" />

**Taxonomy of Techniques**

根据数据的信息丰度将数据分为高资源部分和低资源部分，导致了根据预测目标而定的两个主要任务分支：改进低资源部分，并平衡高/低资源部分

前者寻求提高低资源部分的性能，而不考虑其他部分，后者力求在两个部分之间实现性能平衡



<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016192311040.png" alt="image-20231016192311040" style="zoom:80%;" />

# 4 ILOGS PROBLEMS: CLASS IMBALANCE

## 4.1 Node-Level Class Imbalance

节点级类别不平衡是指标记节点在类别之间的分布不成比例。该模型通常会从具有大量标记实例的类中学习更多信息，但可能会忽略那些具有较少标记实例的类

### 4.1.1 Imbalanced Node Classification

不平衡通常会导致性能偏差，因为大多数（高资源）类由于标记节点数量较多而往往优于少数（低源）类

解决方案

- 算法级

  - DRGCN：基于 GNN 的方法，采用生成对抗网络（GAN）来生成合成节点以平衡类分布，并与 KL-divergence 约束配对以同步表示未标记节点与标记节点的分布
  - DPGNN：采用类原型驱动的训练方法来平衡类之间的训练损失，借助距离度量学习来准确捕获与类原型有关的节点的相对位置
  - TAM：通过考虑少数节点之间同质性的降低来解决类不平衡问题。具体来说，它创新地引入了连接和分布感知边距来指导模型，强调类连接和邻居标签分布
  - LTE4G：考虑了节点类别和度数的不平衡。将节点划分为分配给专家 GNN 的平衡子集，然后采用知识蒸馏来训练特定类别的学生，以提高分类性能

- 数据级

  数据级方法通常旨在通过尾部类过采样、头类欠采样或基于实例的类重新加权来平衡数据分布

  - GraphSMOTE、ImGAGN 都围绕生成合成节点来平衡类
  - GraphMixup、GraphENS、GraphSANN 提供了一种独特的合成节点生成方法

<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016193854749.png" alt="image-20231016193854749" style="zoom:80%;" />

**Summary**

尽管进行了广泛的研究，该领域仍需要进一步探索创新技术，例如使用扩散模型生成合成节点或采用迁移学习将知识从多数类转移到少数类以增强知识，可能是有希望的研究途径

### 4.1.2 Node-Level Anomaly Detection

节点级异常检测作为节点分类不平衡的具体表现，一般以正常节点和异常节点的二元分类为中心



### 4.1.3 Few-Shot Node Classification

在各种现实世界的设置中，经常会遇到新颖的类，其特征是只有少数标记实例或少数实例可用。这种稀缺性给模型训练带来了重大障碍，为了解决这个问题，通常利用一组富含标记数据的基类来协助学习过程，因此，图上的少样本节点分类（FSNC）的主要目标是通过有效地利用从基类派生的信息，在这些少样本新类上产生鲁棒且卓越的性能



### 4.1.4 Zero-Shot Node Classification



## 4.2 Edge-Level Class Imbalance

### 4.2.1 Few-Shot Link Prediction

### 4.2.2 Edge-Level Anomaly Detection

边缘级异常检测旨在识别网络内的异常边缘

### 4.3.2 Graph-Level Anomaly Detection

图级异常检测旨在根据预定义的异常度量来检测异常图或子图，这可以看作是具有二进制标签的不平衡图分类的变体

### 4.3.3 Few-Shot Graph Classification

少样本图分类（FSGC）是图不平衡学习的一个关键方面，旨在将目标图分类为各自的新类别



# 5 ILOGS PROBLEMS: STRUCTURE IMBALANCE

与图像或文本等其他数据类型不同，图数据本质上包含可能表现出不平衡的拓扑结构

## 5.1 Node-Level Structure Imbalance

当每个节点周围的上下文结构表现出不均匀分布时，就会出现节点级结构不平衡。节点层次结构的一个突出指标是节点度，**代表邻居节点的数量**，反映节点的邻近丰富度

![image-20231016195213418](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016195213418.png)

### 5.1.1 Imbalanced Node Degrees

在图中，节点度数通常遵循长尾分布，头节点（度数高的节点）受益于更丰富的结构信息，从而在节点分类等下游任务中实现卓越的性能。相反，度数较低的尾节点的拓扑信息有限，从而影响其性能研究通常针对两个主要任务：提高尾节点的性能，以及解决图中孤立的更具挑战性的冷启动节点

**Tail node embedding**

Demo-Net 和 SL-DSGCN 利用 degree-specific GNNs 来捕获不同程度节点之间的独特结构模式

meta-tail2vec 和 Tail-GNN 实现了知识转移机制。前者利用元学习将知识从头节点转移到尾节点，将尾节点嵌入重新定义为几次嵌入回归任务。同时，后者提出了一种邻域翻译技术来桥接头节点和尾节点以进行知识转移

Residual2Vec 根据节点标签和度数重新衡量损失函数

CenGCN：量化了中心顶点与其邻居顶点之间的相似性，并通过边权重调整和自连接应用图变换，有效缓解了这一问题

BLADE：通过使用有偏邻域采样来应对链接预测中的程度差异问题。通过根据节点连接性创建不同大小的邻域，它优化了不同节点度的性能

DHGAT：通过利用用户历史的一阶和二阶邻近性，利用注意力机制进一步利用目标实体的邻居来实现这一点

hao等人[236]：借鉴 meta-tail2vec 的概念，将回归应用于尾部节点以促进知识转移，从而实现冷启动推荐



**Cold-start node embedding**

在某些极端情况下，图可能包含许多孤立的节点，称为冷启动节点，它们没有邻居，因此对传统的图表示学习方法提出了挑战，通常采用知识蒸馏范式。这些方法利用教师网络在信息充足的节点上进行知识提取，并利用学生网络在冷启动节点上进行知识应用，最终提高了后者的性能

Cold Brew 是一种著名的方法，旨在通过利用知识蒸馏的概念来解决冷启动节点表示学习问题，该方法采用 GNN 网络作为教师网络，从信息充足的节点中提取知识，并采用多层感知器（MLP）作为学生网络，从教师那里提取知识



### 5.1.2 Node Topology Imbalance



### 5.1.3 Long-Tail Entity Embedding on KGs

知识图通常呈现跨实体的三元组长尾分布，这可能会导致 KG 富集等应用中的性能偏差

GEN：



## 5.2 Edge-Level Structure Imbalance

复杂图中的边级结构不平衡（例如 KG）表现为不同的边（关系）频率，通常遵循长尾分布。这种不平衡可能会使模型产生偏差，边缘化长尾关系并导致性能不佳



## 5.3 Graph-Level Structure Imbalance



# 6 TECHNIQUES OF ILOGS

图不平衡问题通常涉及两个主要组成部分：高资源部分和低资源部分。这导致了两个主要任务类别：改进低资源部分和平衡高资源部分和低资源部分

<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016202115276.png" alt="image-20231016202115276" style="zoom:80%;" />

<img src="C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016203017548.png" alt="image-20231016203017548" style="zoom:50%;" />

一些研究可能会结合来自另一个分支的技术来减轻自己领域中的不平衡问题，例如，一些研究[251]应用 Mixup 进行合成数据生成，增强低资源部分的性能，[32]、[63]，使用公共知识共享或知识蒸馏等技术进行知识转移，寻求两个部分之间的平衡性能



## 6.1 Improving the Low-Resource Part

旨在改进低资源部分的任务的主要目标是提高其性能，尽管与高资源部分相比数据较少，为了解决不平衡问题，通常使用两种主要技术：**知识转移和辅助数据的合并**，以利用现有资源来补充资源匮乏的部分

### 6.1.1 Knowledge Transfer

知识转移旨在利用数据丰富源中的知识库，并将其转移到知识缺乏的部分，从而提高其绩效



**Meta-learning**

它从一系列具有共享分布的基本元任务中提取通用的元知识。然后将这些知识应用于新的元任务以增强预测性能，所采用的技术通常包括基于优化的方法（例如，MAML [206]）和基于度量的方法（例如，原型网络[207]）等



**Model pre-training**

“预训练、微调”的范式是机器学习中广泛接受的两步模型训练策略。最初，使用自监督学习在大型数据集上对模型进行预训练，从而揭示数据中嵌入的基础知识。之后，预训练模型在较小的、特定于任务的监督数据集上进行微调



**Knowledge distillation**

知识蒸馏[252]是一种技术，其中较小的“学生”模型通过模仿较大的“教师”模型的行为来学习



**Common knowledge sharing**

共同知识共享构成了知识转移的关键，通过共享元素在高资源部分和低资源部分之间建立了渠道



### 6.1.2 Auxiliary Data

机器学习中的辅助数据是指增强主数据集的补充信息，从而提高模型性能。对于 ILoG，辅助数据通常通过提供额外的知识来支持低资源部分的学习过程



## 6.2 Balancing the High- and Low-Resource Parts

与单独改进低资源部分的目标相反，平衡高资源部分和低资源部分的目标是增强低资源部分的性能，同时最大限度地减少高资源部分的潜在性能下降，为了实现这一目标，提出了三种主要技术：**数据重新加权和重采样、合成数据生成和纳入额外的约束**



### 6.2.1 Data Reweighting and Resampling

数据重采样和重新加权技术通常用于不平衡学习，以解决训练数据中的类别不平衡问题



### 6.2.2 Synthetic Data Generation

合成数据生成技术广泛应用于 ILoG，通过生成用于数据重新平衡的合成数据来解决类不平衡的挑战具体来说，两种常用的合成数据生成技术是 SMOTE（合成少数过采样技术）[16]和 GAN（生成对抗网络）[60]



**SMOTE**

SMOTE [16] 是一种众所周知的方法，用于不平衡学习，以解决训练数据中的类别不平衡问题

对于每个少数类实例，SMOTE 从同一类中选择一个或多个 k 最近邻，并通过在所选实例与其邻居之间进行线性插值来生成合成示例。这个过程有效地扩展了少数类并引入了额外的数据点以提高其在训练期间的表示

此外，Mixup [68]，一种最初提出用于训练深度神经网络的数据增强技术，也可以在不平衡的学习场景中使用。 Mixup 通过实例对的线性组合生成虚拟训练示例，扩展了插值的思想



**GAN**

生成器和鉴别器，以对抗方式一起训练，GAN 因生成与原始数据分布非常相似的真实且高质量的数据（例如图像）而闻名，在 ILoG 的背景下，一些研究（例如 IMGAGN [37]）探索了使用 GAN 为少数类别生成合成样本，其功能与 SMOTE 类似。这些基于 GAN 的技术可以帮助平衡数据分布并提高不平衡图学习场景中少数类的模型性能



### 6.2.3 Additional Constraints

略



## 6.3 Appropriate Techniques Selection

![image-20231016213216182](C:\Users\Asus\AppData\Roaming\Typora\typora-user-images\image-20231016213216182.png)

首先，任务分析涉及了解数据不平衡及其挑战。此步骤需要确定将数据拆分为高资源部分和低资源部分，并确定主要目标：提高低资源部分的性能，或平衡这两个部分。该分析是后续技术选型的基础

其次，技术选择涉及考虑各种方法来解决已识别的不平衡问题。每种技术都提供了解决问题的独特视角和能力，使其成为潜在的候选技术

最后，探索其他分支的技术是值得努力的。一方面，虽然大多数技术都分为两个分支——改进低资源部分或平衡高资源部分和低资源部分——但界限并不严格，即，用于一个分支的技术可能适用于另一分支中的任务

相反，知识转移等技术可以通过将知识从高资源部分转移到低资源部分来促进平衡这两个部分。另一方面，探索 ILoG 中不常用的新技术也可能是有益的，



扩散模型 diffusion models [71]

数据提示 data prompting [291]

基础模型 foundation models [187]



# 7 OTHER RELATED LITERATURE

## 7.1 Fairness Learning on Graphs

在图上实现公平学习的方法主要分为两类：为模型训练添加约束和预处理输入数据



## 7.2 Imbalanced learning in Recommendations

推荐中的不平衡学习主要解决用户项目评分图（二分图）中的冷启动推荐

为了缓解冷启动推荐问题，人们采用了多种技术，包括

元学习[336]、[337]

预训练[338]

知识蒸馏[339]

针对这一挑战，我们鼓励读者查阅相关调查[50]



# 8 Some

为什么推荐系统需要 GNN？

(1) structural data

通过将所有数据表示为图上的节点和边，GNN 提供了一种利用可用数据的统一方法。同时，GNN 在学习表示方面表现出强大的能力，因此可以获得用户、项目和其他特征的高质量嵌入，这对于推荐性能至关重要

(2) high-order connectivity

推荐准确性依赖于捕获用户和项目之间的相似性，并且这种相似性应该反映在学习的嵌入空间中,用户学习到的嵌入类似于用户与之交互的项目的嵌入。此外，那些与具有相似偏好的其他用户交互的项目也与该用户相关，这被称为协同过滤效应，这对于推荐准确性非常重要,基于 GNN 的模型可以有效捕获高阶连接。具体来说，协同过滤效果可以自然地表示为图上的多跳邻居，并通过嵌入传播和聚合将其合并到学习的表示中

(3) supervision signal.

监督信号在收集的数据中通常是稀疏的，而基于 GNN 的模型可以在表示学习过程中利用半监督信号来缓解这个问题



挑战

- 如何为特定任务构建适当的图表
- 如何设计信息传播和聚合的机制
- 如何优化模型
- 如何保证模型训练和推理的效率