# 0. 理解贝叶斯定理

## **条件概率**

​		先要从条件概率讲起，条件概率，一般记作P(A|B)，意思是当B事件发生时，A事件发生的概率。其定义为
$$
P(A|B)=\,{\frac{P(A\cap B)}{P(B)}}
$$
​		其中$$P(A\cap B)$$意思是A和B共同发生的概率，称为联合概率。也可以写作 P(A,B) 或 P(AB)。 注意，定义中A与B之间不一定有因果或者时间序列关系。

1. **样本空间**

​		回顾一下，样本空间是一个实验或随机试验**所有可能结果的集合**。例如，抛掷一枚硬币，那么样本空间就是集合{正面，反面}。如果投掷一个骰子，那么样本空间就是 {1,2,3,4,5,6}。样本空间的任何一个**子集**都被称为一个**事件**。 所以，当我们通常说某个事件的概率时，其实是默认省略了该事件的样本空间。比如说事件A的概率是P(A)，其实是指，在样本空间 **Ω** 中，事件A的数量占**Ω**的比率，记作P(A)。比如说骰子掷出3点的概率是1/6，其实是说，在掷骰子所有可能结果的集合中（样本空间）中，出现事件"3点"（子集）的比率是1/6。也就是 size{3} / size{1,2,3,4,5,6} = 1/6。

2. **条件意味着缩小的样本空间，是二级概率**

​		通常说概率P(A)是针对样本空间 **Ω** 来说的，而条件概率中的条件，比如P(A|B)，意思是事件B发生的情况下，因此非B的样本空间被这个条件排除掉了，所以这时P(A|B)已经不是针对 样本空间 **Ω** 了，而是针对缩小的样本空降 B。

<img src="C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230409102906763.png" alt="image-20230409102906763" style="zoom:50%;" />

​		结合上图来理解。原来样本空间是 **Ω**，事件B发生，意味着样本空间缩小到B的范围，即上图黄色椭圆范围内。同时事件A也发生，也就是上图中 A∩B 蓝色部分，蓝色部分对黄色椭圆的占比，就是条件概率 P(A|B)。可以写作
$$
P(A|B)=\ {\frac{s i z e A\cap B}{s i z e B}}\tag{1}
$$
如果考虑
$$
P(A\cap B)=\frac{s i z e A\cap B}{s i z e\Omega}
$$
$$
P(B)=\,{\frac{s i z e B}{s i z e\Omega}}
$$

所以
$$
P(A|B)=\ {\frac{P(A\cap B)}{P(B)}}\tag{2}
$$


## **贝叶斯定理**

从条件概率出发很容易推导出贝叶斯定理
$$
P(A|B)=\ {\frac{P(A\cap B)}{P(B)}}\tag{3}
$$

$$
P(B|A)=\ {\frac{P(A\cap B)}{P(A)}}\tag{4}
$$

$$
{\frac{P(A|B)}{P(B|A)}}=\ {\frac{P(A)}{P(B)}}\tag{5}
$$

$$
P(A|B)=\ {\frac{P(B|A)P(A)}{P(B)}}\tag{6}
$$

公式(5)可以理解为 条件概率的比值 = **先验概率的比值** = 椭圆A / 椭圆B。（先验概率指P(A)和P(B)，由于不涉及其它条件，即P(A)与B无关，P(B)与A无关，所以称为先验。条件概率在这里又称为后验概率，因为P(A|B)意味着已知B事件发生之后，P(B|A)意味着已知A事件发生之后）。

公式(6)就是通常贝叶斯定理的形式。



# 1. 极大似然估计

**极大似然估计方法（Maximum Likelihood Estimate，MLE）**也称为最大概似估计或最大似然估计，是求估计的方法之一。（还有矩估计等）
  简单来讲，极大似然估计就是给定模型，然后通过收集数据，求该模型的参数。例如，投10次特殊的硬币（给定模型），出现6次正面4次反面（请注意，这里10次结果有顺序，后面所有的投硬币结果，都有顺序）（收集数据），现在要估计投这枚硬币出现正面的概率（求参数）。
  由于提及“投硬币”，一般人的第一印象就是投到正面和反面的概率都是0.5。不过这样不利于我们接下来的学习，这里，我们需要摆脱以往的直觉，该硬币是特殊的，正反的概率并一定是0.5。
  那么，我们根据以上收集的数据先凭借直觉猜一下，很明显，由于投了10次，出现了6次正面，一般人都会猜投硬币出现正面的概率最有可能是0.6，当然也不排除其他的可能。
  而最大似然估计，简单来讲，就是用数学方法来解释你这种直觉，它就是计算出可能性最大的结果。
  上面投硬币的例子，很明显可以看出，这个模型服从二项分布，即进行多次实验，每次实验只有两种可能。用$$x_0，x_1，…，x_9$$表示这10次投硬币的结果，用 $$ θ $$ 表示投该硬币出现正面的概率，那么把我们的直觉写成用数学写出来就是似然函数，可表示成：
$$
f(\theta)=\theta^{\mathrm{6}}\cdot(1-\theta\,)^{4}
$$
当然，不要被**似然函数**吓到了，他就是一个名字而已，就上面那个图片中的函数，相信你能看懂。而最大似然估计，顾名思义，就是要最大化这个似然函数，说得更简单点，就是**给你一个函数，求它的极大值点**。即
$$
\mathop{a r g m a x}_{\theta}\ f(\theta)
$$
 对似然函数取对数，不会影响该函数的单调性，从而不会影响最后的计算的极值，也可以在一定程度上减少因计算而带来的误差，还可以极大的简化计算。
  此时求解取对数的似然函数的极大值点，就是似然函数的极大值点。由于似然函数是先单调递增，然后再单调递减的，因此，取对数的似然函数导数为0的点，即是似然函数或取对数的似然函数的极大值点（在这里也是最大值点）
  最大似然估计可以转化为求下面式子的解：
$$
\mathop{a r g m a x}_{\theta}\ l n f(\theta)
$$
下面简单说一下求解方法：

  这里以上面得例题为例，首先求出最大似然函数对其进行极大似然估计，然后对似然函数取对数，如下所示：
$$
l n\,f(\theta\,)=l n\bigl(\theta^{6}\cdot(1-\theta\,)^{4}\bigr)=6\cdot l n\,\theta\,+4\cdot l n(1-\theta\,)
$$
​		最后对取对数的似然函数取进行求导，在函数的导数为0的点，即为最大似然的估计值，如下所示：
$$
{\frac{d\ l n f(\theta)}{d\theta}}={\frac{6}{\theta}}-{\frac{4}{1-\theta}}=0
$$

$$
\theta=0.6
$$
​		写到这里，我们就得到了该最大似然的估计值，即投该硬币出现正面的概率θ=0.6。
  以上就是最大似然估计的方法，**如果未知参数有多个，则需要用取对数的似然函数对每个参数进行求偏导，使得所有偏导均为0的值，即为该函数的极值点，一般也是其最大似然估计值。**



# 2. 最大后验概率估计

​		对于最大后验概率估计，我们先进行通俗简单的理解，还是以刚才的那个问题为例，投10次硬币，结果分别是$$x_0,x_1,…,x_9$$，出现了6次正面，4次反面。

​		现在，有两个人A和B，其中A觉得那枚硬币，它就是一个一般的硬币，出现正面的概率θ = 0.5。而B觉得，它是一个特殊的硬币，出现正面的概率$$θ$$ = 0.6。

​		最大后验概率就是把他们的假设都进行计算（验算），然后选择其中假设最好的一个，当作最大后验概率。

​		它首先计算A的假设，假设出现正面的概率θ = 0.5，那么此时，投10次该硬币，出现6次正面的概率则是
$$
P(x_{0},x_{1},\ldots,x_{9}|\theta)=\theta^{6}\cdot(1-\theta)^{4}\!=\!{0.5}^{6}\cdot(1-0.5)^{4}\approx0.00097656
$$
​		然后再来计算一下B的假设，假设出现正面的概率θ = 0.6，那么此时，投10次该硬币，出现6次正面的概率则是
$$
P(x_{0},x_{1},\ldots,x_{9}|\theta)=\theta^{6}\cdot(1-\theta)^{4}\!=\!{0.6}^{6}\cdot(1-0.6)^{4}\approx0.00119439
$$
​		通过计算，我们可以很直观的发现，相比于A的假设，B的假设准确的概率更大一下，当然，这里也不能说B绝对是正确的，只是B是对的可能性更大。我们一般也更相信B的猜测。
  当然，上面的例子只是对最大后验概率估计进行简单的理解。显然，我们还可以有其他假设，比如假设出现正面的概率θ = 0.7。由于θ的取值范围在0到1之间，有无数种假设，但我们不可能每种假设都进行计算，这个时候，就需要利用一些简单的数学方法，求出最大的那一个，即为最大后验概率。
  **最大后验概率估计就是在已知一系列结果的情况下，求参数可能的最大的那一个，也就是求解下面式子**：
$$
\mathop{a r g m a x}_{\theta}\ P(\theta|x_0,x_1,...,x_n)
$$
​		可能有人不知道为什么要写这个式子，我们来简单通俗的理解一下上面式子的含义，就是在序列已知的情况下，θ等于某个值的概率（这里不懂的，参看第一节），然后求出θ一个个的取完所有的值的所有概率，选择其中使概率最大的那一个的θ，即为最大后验概率。
  然而该式一般不能通过蛮力法直接求解，需要利用贝叶斯公式，经过一系列变换求解，过程如下：
$$
P(\theta|x_{0},x_{1},\ldots,x_{n})={\frac{P(x_{0},x_{1},\ldots,x_{n}|\theta)\times P(\theta)}{P(x_{0},x_{1},\ldots,x_{n})}}
$$
​		上面这个式子中$$P(\theta|x_{0},x_{1},\ldots,x_{n})$$表示投n次硬币，产生$$x_0 , x_1 , … , x_n$$结果的概率，其中每个x均有两种可能，要么是正面，要么是反面。但是由于已经投了n次硬币，统计了每一次的结果，即$$x_0 , x_1 , … , x_n$$ 均已知，相当于这个事件已经发生了，所以在这里$$P(x_{0},x_{1},\ldots,x_{n})$$ = 1。
  还需要注意的是，这里的$$P(x_{0},x_{1},\ldots,x_{n}|\theta)$$就是前面第二节中极大似然估计的似然函数。这里的$$P(θ)$$表示的是出现正面的概率，简单理解就是，我们最开始就需要对θ设置一个概率分布，例如可以假设它在0到1之间均匀分布，即它可以等概率的取0到1之间任意值，此时，最大后验概率估计等价于最大似然估计。假如我们不考虑太多，就觉得该硬币的出现正面的概率最接近0.5，与0.5相差越大则越不可信，那么可以将θ的概率分布假设为均值为0.5，方差为σ^2的正态分布。
  所以该题中的最大后验概率估计，也等价于下式
$$
\mathop{a r g m a x}_{\theta}\ P(\theta|x_0,x_1,...,x_n) = \mathop{a r g m a x}_{\theta}\ P(x_0,x_1,...,x_n|\theta)\times P(\theta)
$$
​		对于刚才的投硬币问题，我们将θ的概率分布假设为均值为0.5，方差为1的**正态分布**，即θ的密度函数可表示为：
$$
f(\theta)=\frac{1}{\sqrt{2\pi}}e^{-\frac{(\theta-0.5)^{2}}{2}}
$$
​		最大后验概率的求解与最大似然估计有些类似，也是先对函数取对数，然后再求导数为0的极值点，即为最大后验估计的概率。

​		显然在这道题中θ = 0.5977，也就是说，当θ的密度函数为均值为0.5，方差为1的正态分布时，投该硬币出现正面的概率为0.5977时是可能性最大的。
  说到这里，最大后验概率估计的方法，也差不多说完了。下面按照我所学，以容易理解的方式总结一下：
  假设你已经理解了极大似然估计，那么，**最大后验的实质就是对参数的每一个可能的取值，都进行极大似然估计，并根据这个取值可能性的大小，设置极大似然估计的权重，然后选择其中最大的一个，作为最大后验估计的结果。**



**最大似然估计 vs 最大后验概率估计**

最大后验概率估计其实就是多了一个参数的先验概率，也可以认为最大似然估计就是把先验概率认为是一个定值；

为什么先验概率加进去有用？

比如抛硬币，先验概率认为，正面朝上概率的最大可能是 0.5；但是在实际抛硬币的过程中，可能会出现抛 10 次 7 次正面的情况，这种情况下，最大似然估计会认为正面朝上的概率最大可能是 0.7，最大后验概率估计会综合考虑实验和先验概率，因此认为正面朝上的概率最大可能是 0.5-0.7 之间的一个值，相当于先验概率对其做了一个校正；

最大似然估计可以认为是频率学派的观点，最大后验概率估计可以认为是贝叶斯学派的观点；

**后验概率 := 似然 \* 先验概率**



# 3. BPR

## 什么是BPR算法

BPR即Bayesian Personalized Ranking，中文名称为贝叶斯个性化排序，是当下推荐系统中常用的一种推荐算法。与其他的基于用户评分矩阵的方法不同的是BPR主要采用用户的隐式反馈（如点击、收藏等），通过对问题进行贝叶斯分析得到的最大后验概率来对item进行排序，进而产生推荐。



## BPR算法的基本原理

### 1.符号定义

**U**：用户集

**I**：物品集

**S**：
$$
\ S\subseteq U\times I
$$
![image-20230409203121436](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230409203121436.png)

有过隐式反馈记录的为“+”，反之为“？”。

**>u**：用户u的偏好
$$
i>_{u}j
$$
![image-20230409203436186](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230409203436186.png)

![image-20230409203441326](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230409203441326.png)





假设A是一个集合 {1,2,3} ；R是集合A上的关系，例如{<1,1>,<2,2>,<3,3>,<1,2>,<1,3>,<2,3>}

- 自反性：任取一个A中的元素x，如果都有<x,x>在R中，那么R是自反的。
- 对称性：任取一个A中的元素x,y，如果<x,y> 在关系R上,那么<y,x> 也在关系R上，那么R是对称的。
- 反对称性：任取一个A中的元素x,y(x!=y)，如果<x,y> 在关系R上,那么<y,x> 不在关系R上，那么R是反对称的。
- 传递性：任取一个A中的元素x,y,z，如果<x,y>,<y,z> 在关系R上，那么 <x,z> 也在关系R上，那么R是对称的。



偏序： 设R是非空集合A上的关系，如果R是自反的，反对称的，和传递的，则称R是A上的偏序关系。

全序：如果R是A上的偏序关系，那么对于任意的A集合上的 x,y，都有 x <= y，或者 y <= x，二者必居其一，那么则称R是A上的全序关系。



所以可以看到，全序也是一种偏序。偏序究竟在说啥，关键在于反对称性上，就是说，<x,y> 在关系R上，那么 <y,x> 不在关系R上，那我问你，<y,x> 关系是啥，就是大家都不知道。所以说偏序就在于你的集合A={1,2,3,4}，有一些元素的关系根据R你是得不出的。那么既然你不知道这个<y,x>，那么全序关系上，就多加一个条件，都有 x <= y，或者 y <= x，二者必居其一，这样你总知道了吧。



**偏序举例**：

假设有 A={1,2,3,4}，假设R是集合A上的关系：

{<1,1>,<2,2>,<3,3>,<4,4>,<1,2>,<1,4>,<2,4>,<3,4>}，那么：

1. 自反性：可以看到 <1,1>,<2,2>,<3,3>,<4,4> 都在R中，满足。
2. 反对称性：由于 <1,1>,<2,2>,<3,3>,<4,4> 不属于 x !=y ，所以不考虑这4种，对于 <1,2>，有 <2,1> 不在R中；对于<2,4> 有<4,2>不在R中；对于<3,4> 有<4,3> 不在 R中，满足。
3. 传递性：<1,1><1,2>在R中，并且<1,2>在R中；<1,1><1,4>在R中，并且<1,4>在R中；<2,2><2,4>在R中，并且<2,4>在R中；<3,3><3,4>在R中，并且<3,4>在R中；等等其他，满足。

所以说R是偏序关系。



**全序举例**：

假设有 A={a,b,c}，假设R是集合A上的关系：

{<a,a>,<b,b>,<c,c>,<a,b>,<a,c>,<b,c>}

和上述一样，可以证明具有自反性，反对称性，传递性，所以是偏序的，有因为有 <a,b>,<a,c>,<b,c>， 也就是说两两关系都有了，所以满足对于任意的A集合上的 x,y，都有 x <= y，或者 y <= x，二者必居其一，所以说是全序关系。



### 2.训练数据的预处理

首先对隐式反馈矩阵进行预处理

![image-20230409205503224](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230409205503224.png)

得到训练数据集
$$
D_{S}:=\{(u,i,j)|i\in I_{u}^{+}\wedge j\in I\setminus I_{u}^{+}\}
$$
其中的元素
$$
(u,i,j)\in D_{S}
$$
表示对于u来说，在i和j之间更偏好i



### 3.两个基本假设

- 每个用户之间的偏好行为相互独立，即用户u在商品i和j之间的偏好和其他用户无关。
- 同一用户对不同物品的偏序相互独立，也就是用户u在商品i和j之间的偏好和其他的商品无关。



### 4.BPR的推导

BPR算法的基本思想是利用最大化后验概率来确定所有item
$$
i \in I
$$
的正确个性化排序，我们不妨假设正确的个性化排序可以由某个模型（如矩阵分解）给出，而θ表示这个模型的参数向量。于是我们优化的目标就是使得
$$
p(\Theta\vert>_{u})
$$
最大，进而求出θ。

根据贝叶斯公式得 $$P(\theta\mid\ >\ u)=(P(>\ u\mid\theta)P(\theta))/(P(>u)) $$

因为BPR假设用户的排序和其他用户无关，那么对于任意一个用户u来说，P(>u)对所有的物品都为一个常数，所以最大化
$$
p(\Theta\vert>_{u})
$$
等价于最大化
$$
p(\Theta\vert>_{u})p(\Theta)
$$
即
$$
p(\Theta|>_{u})\propto p(>_{u}|\Theta)\,p(\Theta)
$$
这个最大化目标可以分为两部分，左边的部分与数据集有关，右边部分P（θ）与数据集无关。

我们先最大化左边的部分$$P(>\,u|\theta) $$
$$
\prod_{u\in U}p(>_{u}|\Theta)=\prod_{(u,i,j)\in U\times I\times I}p(i>_{u}j|\Theta)^{\delta((u,i,j)\in D_{S})} \cdot(1-p(i>_{u}j|\Theta))^{\delta((u,j,i)\neq D_{S})} 
$$

$$
\delta(b):={\left\{\begin{array}{l l}{1}&{{\mathrm{if~}}b{\mathrm{~is~true}},}\\ {0}&{{\mathrm{else}}}\end{array}\right.}
$$

根据完整性和反对称性约束，上式可以简化成
$$
\prod_{u\in U}p(>_{u}|\Theta)=\prod_{(u,i,j)\in D_{S}}p(i>_{u}j|\Theta)
$$
而对于
$$
p(i>_{u}j|\Theta)
$$
这个概率，可以使用下面这个式子来代替:
$$
p(i>_{u}j|\Theta):=\sigma(\hat{x}_{u i j}(\Theta))
$$

$$
\sigma(x):={\frac{1}{1+e^{-x}}}
$$

事实上$$\sigma(x)$$ 也可以取其他满足完整性、反对称性和传递性三个约束条件的其他函数，BPR的最先提出者使用sigmod函数是为了方便接下来的计算。
$$
\hat{x}_{u i j}(\Theta)
$$
同样可以是任何关于模型参数向量θ的实值函数，前提是该函数能映射出用户u与物品i和j之间的关系。

于是第一部分的优化目标最终变成了
$$
\prod_{(u,i,j)\in D_{\mathcal{S}}}\sigma(\hat{x}_{u i j})
$$
接下来我们再对第二部分P（θ）进行最大化，由于θ的分布是未知的，为了方便我们的计算，不妨假设其服从均值为0，协方差矩阵为
$$
\lambda_\Theta I
$$
的正态分布

于是最大化
$$
p(\Theta\vert>_{u})
$$
![image-20230409220501553](C:\Users\Azure\AppData\Roaming\Typora\typora-user-images\image-20230409220501553.png)



















